{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "253px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"classification\"></a>\n",
        "# Classification"
      ],
      "metadata": {
        "id": "7cmqMBmoc2bX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## General problem definition\n",
        "\n",
        "- Common supervised learning problem\n",
        "- Learn a model (hypothesis) that is a function  $h:X\\rightarrow Y$ associating an entity (**measurement**) $x \\in X$ with a **label** $y \\in Y$ representing distinct classes of entities -> classification (decision)\n",
        "- Have a dataset in the form of pairs of $ (x_1,y_1),...,(x_n,y_n)$ (measurement-label pairs)\n",
        "- Assume pairs drawn from $P(x,y)$ joint distribution as \"i.i.d\" sample, and  $x_i \\in \\mathbb{R}^d$.\n",
        "\n",
        "- For **binary (single class)** decisions $y_i \\in \\{0,1\\}$\n",
        "- For **multi class** decisions $y_i \\in  \\{0,1,...,k-1\\}$.\n",
        "- Frequent way of encoding multi class: **\"one-hot\"** or \"1-of-K\" vector in the form of: $y_i=(0,0...,1,...0,0) $\n"
      ],
      "metadata": {
        "id": "QoCz1DJ0c63J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some terms for $x$, $y$:\n",
        "\n",
        "* $x$: the measurements along one or more dimensions that serve as the basis for classification. Terms:\n",
        "  - input\n",
        "  - independent variable(s)\n",
        "  - explanatory variable(s)\n",
        "  - ...\n",
        "\n",
        "* $y$: the target of the classification. Terms:\n",
        "  - label\n",
        "  - class\n",
        "  - output\n",
        "  - target\n",
        "  - dependent variable\n",
        "  - explained variable\n",
        "  - ...\n",
        "\n",
        "(Same terms in regression task, except that the target is not a 'label' or 'class'.)"
      ],
      "metadata": {
        "id": "QtkauFE-fB2Q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZZUkPeS4AkT"
      },
      "source": [
        "## Most basic classifier: kNN\n",
        "\n",
        "\"You can get to know a bird from it's feathers and a man from his friends\" - Hungarian proverb\n",
        "\n",
        "Idea: \n",
        "- Examine $k$ nearest data points in neighbourhood, and **by majority voting** decide on class to choose. \n",
        "- If the majority of neighbours are class $1$, that's the class selected.\n",
        "\n",
        "<a href=\"https://cdn-images-1.medium.com/max/1000/0*Sk18h9op6uK9EpT8.\"><img src=\"https://drive.google.com/uc?export=view&id=12rjlmd7CDNWjSliNneS-i0ZdJf3RQdrj\" width=600 heigth=600></a>\n",
        "\n",
        "[Implementation in Python](https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pro:**\n",
        "- Very simple (it's a query in a database)\n",
        "- Supports \"online learning\"\n",
        "\n",
        "**Con:**\n",
        "- Model can become _huge_  (model is the database)\n",
        "- Generalization??"
      ],
      "metadata": {
        "id": "GyDO9rGFeO-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Hint: do not confuse the **_k_**'s:\n",
        ">  * k-means clustering -> number of **clusters**\n",
        ">  * k-nearest-neigbours -> number of closest neighbouring **samples** to take"
      ],
      "metadata": {
        "id": "XW_Us3Bu97aH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Most basic decision boundary\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1iKinTe1NnMeXFik4SLMAU22oi1C2s4jw\"><img src=\"https://drive.google.com/uc?export=view&id=1Wsllvas-txY7icuHWFMdRZSuYD0p4IM-\"  width=900 heigth=900></a>\n"
      ],
      "metadata": {
        "id": "qKkN_lwfedYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Task more formally:** \n",
        "\n",
        "We are looking for a vector in space that defines the direction of the linear decision boundary which separates the two areas.\n",
        "The decision boundary is perpendicular to the weight vector, plus we have a \"step function\" (\"unit step\" or sigmoid) which defines the whole model. We use a closed form solution or iterative approximation to learn the values.\n",
        "(A detailed analysis of this type of model will be pertinent when it comes to *Perceptrons*.)\n"
      ],
      "metadata": {
        "id": "DyED6abPee7u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic regression\n",
        "\n",
        "Confusingly, we talk about \"logistic *regression*\" in this case, though we are learning the most basic of *classifiers*! The reason for the name:\n",
        "\n",
        "<img src=\"http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1534281070/linear_vs_logistic_regression_edxw03.png\">\n",
        "\n",
        "> Q: In the picture above, where is the *decision boundary*? Note that the X-axis encodes the (in this case single) input, and Y the output.\n",
        "\n",
        "[An introduction can be found here, for example.](https://www.datacamp.com/community/tutorials/understanding-logistic-regression-python)\n",
        "\n"
      ],
      "metadata": {
        "id": "CQ1DXhKq68vd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Probabilistic interpretation\n",
        "\n",
        "Suppose the following:\n",
        "- One input variable X\n",
        "- Need to decide between two classes: \"background\" and \"signal\"\n",
        "- Have to chose boundary->value of x where we classify an observation as background vs. signal\n",
        "- Below example is from  physics, from the Large Hadron Collider: \n",
        "    - \"Background\"->background noise from particle collision\n",
        "    - \"Signal\" -> signal for particle to be detected (Higgs-Boson)\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1c9uAoI_lhIgAbPwKWEaeYXWtF79FCVmh\"><img src=\"https://drive.google.com/uc?export=view&id=1AtK3zwFjMHC5uzcwmwhp7VFbGH5t3-8h\" width=600 heigth=600></a>\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1a-uFsPgY9Lz5duOGukiuimP1S_e5dRl4\"><img src=\"https://drive.google.com/uc?export=view&id=1aB1jugknWcMOexFA3lkhQbfmId4FFo2s\" width=600 heigth=600></a>\n",
        "\n",
        "Source: Sergey Gleyzer's Course: \n",
        "Feature Extraction, End-end Deep Learning and Applications to Very Large Scientific Data: Rare Signal Extraction, Uncertainty Estimation and Realtime Machine Learning Applications in Software and Hardware at DeepLearn2018 Genova\n",
        "\n",
        "(Again connection with [Fisher information](https://en.wikipedia.org/wiki/Fisher_information#Matrix_form))\n"
      ],
      "metadata": {
        "id": "C97MjpkmewXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stability of boundary: Max margin classification\n",
        "\n",
        "There are potentially infinitely many decision boundaries that can separate this data. Which one to choose?\n",
        "\n",
        "<a href=\"https://cdn-images-1.medium.com/max/1500/1*UGsHP6GeQmLBeteRz80OPw.png\"><img src=\"https://drive.google.com/uc?export=view&id=1iWL1-DVdj6R6R07LzePDbGmiVHDQWe_o\" width=600 heigth=600></a>\n",
        "[Support Vector Machines — A Brief Overview](https://towardsdatascience.com/support-vector-machines-a-brief-overview-37e018ae310f)\n"
      ],
      "metadata": {
        "id": "Oi8rNg82lrYN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution: Choose the classifier with the maximum \"margin\".**\n",
        "\n",
        "<a href=\"https://nlp.stanford.edu/IR-book/html/htmledition/img1260.png\"><img src=\"https://drive.google.com/uc?export=view&id=1lvKscum0uAOBOCgRx9lSawkVRjKhxLhB\" width=400 heigth=400></a>\n",
        "\n",
        "Intuition:\n",
        "- Most \"**confident**\" classification\n",
        "- Most **tolerant to noise**: needs great amount of perturbation to switch classes (this notion is still important for deep learning, especially [\"adversarial examples\"](http://www.mdpi.com/1999-5903/10/3/26/htm))\n"
      ],
      "metadata": {
        "id": "4PiDGBhfluFe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Support vector machines (Linear)\n",
        "\n",
        "[This](http://www.saedsayad.com/support_vector_machine.htm) is a good description.\n",
        "\n",
        "\n",
        "<a href=\"http://www.saedsayad.com/images/SVM_optimize_3.png\"><img src=\"https://drive.google.com/uc?export=view&id=14aLgNu8Vh5TjAi4D_h1-fEwLgc7LDSgr\" width=600 heigth=600></a>"
      ],
      "metadata": {
        "id": "nVUP6EWgeNOI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQIq-Gkc4AkT"
      },
      "source": [
        "**Where does the SVM objective come from?**\n",
        "\n",
        "We need to find the maximal M, for which there is $\\mathbf w$, $b$ such that \n",
        "$$\n",
        "    \\forall \\mathbf x_i: \\frac{1}{\\|\\mathbf w\\|} y_i(\\mathbf w \\mathbf x_i + b) \\geq M\n",
        "$$\n",
        "from which \n",
        "$$\n",
        "\\forall \\mathbf x_i: y_i(\\mathbf w \\mathbf x_i + b) \\geq \\|\\mathbf w\\|\\cdot M\n",
        "$$\n",
        "Now since if a $\\mathbf w, b$ pair maximizes the margin, for any $\\lambda$ scaling factor $\\lambda \\mathbf w, \\lambda b$ will maximize it as well, we can choose the $\\|\\mathbf w\\|$ in our condition to be $\\frac{1}{M}$ and then the condition becomes\n",
        "$$\n",
        "\\forall \\mathbf x_i: y_i(\\mathbf w \\mathbf x_i + b) \\geq 1.\n",
        "$$\n",
        "Our task in this formulation is to maximize the margin, which is $\\frac{1}{\\|\\mathbf w\\|}$, so, in effect, we have to minimize $\\|\\mathbf w\\|$."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gist of the idea:** \n",
        "\"The beauty of SVM is that if the data is linearly separable, there is a unique global minimum value. An ideal SVM analysis should produce a hyperplane that completely separates the vectors (cases) into two non-overlapping classes. However, perfect separation may not be possible, or it may result in a model with so many cases that the model does not classify correctly. In this situation SVM finds the hyperplane that maximizes the margin and minimizes the misclassifications.\""
      ],
      "metadata": {
        "id": "iq0xsfvDqLIG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Uses [\"Hinge loss\"](https://en.wikipedia.org/wiki/Hinge_loss) or misclassification error\n",
        "\n",
        "\"For an intended output $t = ±1$ and a classifier score $y$, the hinge loss of the prediction $y$ is defined as\n",
        "\n",
        "$$ \\ell (y)=\\max(0,1-t\\cdot y)$$\n",
        "\n",
        "\n",
        "Note that $y$ should be the \"raw\" output of the classifier's decision function, not the predicted class label...\n",
        "It can be seen that when t and y have the same sign (meaning y predicts the right class) and $ |y|\\geq 1$, the hinge loss $ \\ell (y)=0$, but when they have opposite sign, $\\ell (y)$ increases linearly with $y$ (one-sided error).\"\n",
        "\n",
        "\n",
        "For detailed description of the solution, see [this video](https://www.youtube.com/watch?v=IOetFPgsMUc)."
      ],
      "metadata": {
        "id": "pWRqLjf0pgIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pro's and con's\n",
        "\n",
        "**Pro:**\n",
        "- Accuracy\n",
        "- Works well on smaller cleaner datasets\n",
        "- It can be more efficient because it uses a subset of training points\n",
        "- In linearly separable cases it has guarantees and closed form solution\n",
        "\n",
        "**Con:**\n",
        "- Isn’t suited to larger datasets as the training time with SVMs can be high\n",
        "- Less effective on noisier datasets with overlapping classes\n",
        "\n",
        "[source](https://www.kdnuggets.com/2016/07/support-vector-machines-simple-explanation.html)\n",
        "\n",
        "**SVMs were [very successful classifiers](https://arxiv.org/pdf/1802.05319.pdf) (especially bacause of the _\"kernelized\"_ versions) and sometimes they are used even recently [in combination with deep learning methods](http://www.cs.toronto.edu/~tang/papers/dlsvm.pdf).**\n",
        "\n"
      ],
      "metadata": {
        "id": "zvtQJFs3ppdq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What if decision boundary is not linear? - 1. General Linear Models\n",
        "\n",
        "For a human still simple, but for the linear model?\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1TXhITGWK7dnrwOZOxklWy0Sc9iZJwQzf\"><img src=\"https://drive.google.com/uc?export=view&id=1KQzCIirM_6WcSSpAWdICQqGj40b_ue1c\" width=900 heigth=900></a>\n"
      ],
      "metadata": {
        "id": "MQN1WuuOqkYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- Adjust expressive power of model (more on that in the regression class)\n",
        "- Idea of expressive power: number and types of relationships model can represent\n",
        "\n",
        "-> **Include polynomial terms** (in this case second order is good enough) into logistic regression\n",
        "\n",
        "-> **Polynomial**: equations containing exponents with different powers (degree of polynomial= highest power)\n",
        "\n",
        "-> With this we are using \"**generalized linear models**\", since the extended data set is still linearly separable\n",
        "\n",
        "Famous example by Andrew Ng in his Machine Learning class:\n",
        "\n",
        "<a href=\"https://raw.githubusercontent.com/ritchieng/machine-learning-stanford/master/w3_logistic_regression_regularization/logistic_regression_boundaries3.png\"><img src=\"https://drive.google.com/uc?export=view&id=1zzaCe7qPBpGZws9QLPv_u2dzZzNEZMFU\" widht=400 heigth=400></a>\n"
      ],
      "metadata": {
        "id": "_ZKMH2vHrCUb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**The choice of appropriate polynomial terms is a big question and can be ambiguous.**\n",
        "\n",
        "<a href=\"http://journals.plos.org/ploscompbiol/article/figure/image?download&size=large&id=info:doi/10.1371/journal.pcbi.1000173.g006\"><img src=\"https://drive.google.com/uc?export=view&id=1XfAzdCA86ZFjQQlTO86A2Orp5ZZwYmtN\" width=\"100%\"></a>\n",
        "\n",
        "(See also in regression.)\n",
        "\n",
        "(Polynomial idea also relevant in the discussion of kernels in representation learning.)\n"
      ],
      "metadata": {
        "id": "CYK08c2drgCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What if decision boundary is not linear? - 2.  Decision trees \n",
        "\n",
        "[Introduction to decision trees](https://medium.com/machine-learning-101/chapter-3-decision-trees-theory-e7398adac567) and [Decision Tree Classifiers: A Concise Technical Overview](https://www.kdnuggets.com/2016/10/decision-trees-concise-technical-overview.html)\n",
        "\n",
        "Decision trees:\n",
        "- Graph representations of iterative decisions which take into account **one attribute** of $x$ at a time to separate the data into subspaces \n",
        "- Aiming to be **more homogeneously containing one target label**\n",
        "- Represents a **complex, locally linear decision boundary** across all attributes of $x$.\n",
        "\n",
        "<a href=\"https://lh4.googleusercontent.com/5Mf4fkdpBuL-H9JRKfbWk1gOm2uJSRmi4lek5py8uACCmVuudIKR8H2nuNisMYsdH1Uk3j2NJkSk9Zy9JJ8cjGxxNU68NHB7jt8N2QinO2XjXsF-1X0NVcYSkWWtxDKh6dXKH2vxtQ\"><img src=\"https://drive.google.com/uc?export=view&id=1_yCxVlx8olBECMDmIC7cdAA28JnZfFeY\" width=600 heigth=600></a>\n",
        "\n",
        "<a href=\"https://lh6.googleusercontent.com/ozEs-AbYk4YEZ6j1tvDYmRjRXDKdZxtCJCDn2XqqGLRNYZ4Rr6SCYAYzA0RBynxO63LJjMoVK24Y0h9nmT2g5C4_YAv0OaGg86kyt9jXs_465fYVmtpkoJkny3V4hZ8bjJilDJbqgg\"><img src=\"https://drive.google.com/uc?export=view&id=1Udf902dk4RP6nCn2Wm25Dj04-PrtHM22\" width=600 heigth=600></a>\n"
      ],
      "metadata": {
        "id": "p6pIGEw1sHPC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How do we learn it?\n",
        "\n",
        " - Iteratively trying to come up with individual decision criteria that partition the data into subsets with respect to class labels\n",
        " - \"More homogeneous\" neighborhoods (from the perspective of class labels)\n",
        " - Basic algorithm relies on concept of \"information gain\" to judge the most fruitful decision given a certain subset of data\n",
        " - Algorithm chooses the signal attribute for which decision leads to biggest gain of information with regard to separating  labels\n",
        "- Forms single node in the decision tree\n",
        "- For the two subsets \"under\" the node, search for information gain is repeated, if gain is no longer possible (threshold),  procedure stops\n",
        "\n",
        "<a href=\"https://www.kdnuggets.com/wp-content/uploads/dt-induction-algo.jpg\"><img src=\"https://drive.google.com/uc?export=view&id=14S37zmSBje3fgaFljwzjbtNlul6gkTSk\" width=500 heigth=500></a>\n"
      ],
      "metadata": {
        "id": "HAgxoEEFscUz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Information gain](https://en.wikipedia.org/wiki/Decision_tree_learning#Information_gain)\n",
        "\n",
        "Information gain based on concept of **entropy** from information theory\n",
        "\n",
        "Entropy is defined as:\n",
        "\n",
        "$$ H(T)=I_{E}(p_{1},p_{2},...,p_{J})=-\\sum _{i=1}^{J}p_{i}\\log _{2}^{}p_{i}$$\n",
        "\n",
        "where $ p_{1},p_{2},...$ are fractions (probabilities) that add up to $1$ and represent the percentage of each class present in the child node that results from a split in the tree.\n",
        "\n",
        "- Entropy **measures degree of chaos** in possibility space (all possible outcomes)\n",
        "- **The more certain** it is whether a possible state of the world obtains or not **lowers the entropy**\n",
        "- The more uncertain it is whether a possible state of the world obtains, the higher the entropy\n",
        "- -> For classification: if we **know** that certain class is true or false, entropy is **low**, if we are **uncertain** whether a particular class is true or false, enropy is **high**\n",
        "\n",
        "$$ \\overbrace {IG(T,a)} ^{\\text{Information Gain}}=\\overbrace {H(T)} ^{\\text{Entropy(parent)}}-\\overbrace {H(T|a)} ^{\\text{Weighted Sum of Entropy(Children)}}$$ \n",
        "\n",
        "$$ =-\\sum _{i=1}^{J}p_{i}\\log _{2}{p_{i}}-\\sum _{a}{p(a)\\sum _{i=1}^{J}-Pr(i|a)\\log _{2}{Pr(i|a)}}$$ \n",
        "\n",
        "- Information gain used to decide **which feature to split on** at each step in building the tree\n",
        "- Simplicity is best: want to **keep our tree small**\n",
        "- At each step we should **choose the split that results in the purest daughter nodes**\n",
        "\n"
      ],
      "metadata": {
        "id": "x3lJNek_tGwh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### [GINI impurity](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity)\n",
        "\n",
        "- Example for fact that entropy can be measured in more than one way\n",
        "- Measure of **how often a randomly chosen element** from the set **would be incorrectly labeled if it was randomly labeled according to the distribution of labels** in the subset\n",
        "- Gini impurity can be computed by summing the probability $p_{i}$  of an item with label $ i$  being chosen times  probability $ \\sum _{k\\neq i}p_{k}=1-p_{i}$ of a mistake in categorizing that item. \n",
        "- Reaches its **minimum (zero) when all cases in the node fall into a single target category**.\n",
        "\n",
        "To compute Gini impurity for a set of items with $ J$ classes, suppose $ i\\in \\{1,2,...,J\\}$, and let $ p_{i}$ be the fraction of items labeled with class $i$ in the set.\n",
        "\n",
        "$$ I_{G}(p)=\\sum _{i=1}^{J}p_{i}\\sum _{k\\neq i}p_{k}=\\sum _{i=1}^{J}p_{i}(1-p_{i})=\\sum _{i=1}^{J}(p_{i}-{p_{i}}^{2})=\\sum _{i=1}^{J}p_{i}-\\sum _{i=1}^{J}{p_{i}}^{2}=1-\\sum _{i=1}^{J}{p_{i}}^{2}$$ \n",
        "\n"
      ],
      "metadata": {
        "id": "OrstBIXutkin"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Important notes:**\n",
        "\n",
        "- Not difficult to see how the generation process for decision trees is connected to kNN-s and clustering algorithms.\n",
        "\n",
        "- Since decison trees are representing an ordered graph of simple decisions, they can be considered as a ruleset, so they are eminent examples of **\"rule based learners\"**. \n",
        "\n",
        "- Advantage of being interpretable and convertible to structured code.\n",
        "\n",
        "- Advanced and _heavily_ extended forms of decision trees can be extremely well perfoming classifiers and can in certain cases **keep up even with modern deep learning models**.\n"
      ],
      "metadata": {
        "id": "Ckpf1ExUueq5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### An ensemble extension: Random Forest\n",
        "\n",
        "> A random forest is a meta estimator that fits a **number of decision tree classifiers** on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and **control over-fitting**. \n",
        "\n",
        "Pros of Decision Tree over Random Forest:\n",
        "\n",
        "* quicker to train\n",
        "* easier to interpret\n",
        "\n",
        "Pros of Random Forest over Decision Tree:\n",
        "\n",
        "* reduce risk of [overfitting](https://zhangruochi.com/Overfittingin-decision-trees/2019/05/11/) on the training data\n",
        "  * -> on unseen data, better than decision tree!\n",
        "\n"
      ],
      "metadata": {
        "id": "nqcoP7IfDsDJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to measure classification\n",
        "\n",
        "- Main goal is to learn \"most accurate\" model.\n",
        "- Can mean multiple things in the case of classification.\n",
        "- Good walkthrough can be found [here](https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9), we will follow the example:\n",
        "\n",
        " Supposed we have the following summary table for our model output, called **\"confusion matrix\"**:\n",
        "\n",
        "<a href=\"https://cdn-images-1.medium.com/max/1200/1*TS2hsRr528UHQG9nDJhIcA.jpeg\"><img src=\"https://drive.google.com/uc?export=view&id=167umGQtt2kI8rtK6OYk6JV00DS-BNHYB\" width=400 heigth=400></a>\n",
        "\n",
        "- Model has 99.9% accuracy\n",
        "- Relies heavily on **class imbalance** of data\n",
        "- **Class imbalance** - some classes much more frequent than others\n",
        "\n",
        "-> Accuracy not a good estimate of model's true performance.\n",
        "\n",
        "-> Simply by \"betting\" on the more acccommon class, model may be right\n",
        "\n",
        "(A _very_ instructive description of a case about class imbalance and it's remedies can be read [here](https://towardsdatascience.com/detecting-financial-fraud-using-machine-learning-three-ways-of-winning-the-war-against-imbalanced-a03f8815cce9).)\n",
        "\n",
        "We have to introduce two additional metrics:\n"
      ],
      "metadata": {
        "id": "p6AWKeKDuo7X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Precision** and **Recall**\n",
        "\n",
        "<a href=\"https://cdn-images-1.medium.com/max/1200/1*OhEnS-T54Cz0YSTl_c3Dwg.jpeg\"><img src=\"https://drive.google.com/uc?export=view&id=12kzMU9zipKV1Vtv0y2gRjMFwCR0HfK7G\" width=400 heigth=400></a>\n",
        "\n",
        "<a href=\"https://cdn-images-1.medium.com/max/1200/1*7J08ekAwupLBegeUI8muHA.png\"><img src=\"https://drive.google.com/uc?export=view&id=1MBw-jog-uhqkeFFsEG0VBXQxEJt90hE1\" width=400 heigth=400></a>\n",
        "\n",
        "> **Precision** tells us about *how many we got right of predicted positive class*. (\"How right we are when we say something is positive.\")\n",
        "\n",
        "> **Recall** tells us about *how many of the actual positives we \"found\"*.\n",
        "\n",
        "If we consider the original example in this light, things are not that rosy:\n",
        "\n",
        "* Precision = 1 / (1+0) = 100%, but\n",
        "* Recall = 1 / (1+1) = 50%."
      ],
      "metadata": {
        "id": "yyve4rpuvi0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important note:**\n",
        "\n",
        "**Depending on situation, it is _not_ true that precision and recall are equally important! The cost of misclassification can change dramatically!** \n",
        "\n",
        "E.g.\n",
        "\n",
        "Banking fraud case: \n",
        "- if we label many transactions as suspicious, but it turns out, that only half of them really are, but _all_ fraudulent ones are in this set, we have done a marvelous job \n",
        "- high recall and less precision was good\n",
        "\n",
        "Molecule prediction: \n",
        "- We don't care how many _other_ molecules would be suitable for a specific task.\n",
        "- Recall is irrelevant, but we would like to be very precise in our prediction that a molecule will be suitable for a specific purpose."
      ],
      "metadata": {
        "id": "QYyjh0MXwZZk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### F1\n",
        "\n",
        "If we would like to have a more balanced metric, we have to utilize **F1** score:\n",
        "\n",
        "<a href=\"https://cdn-images-1.medium.com/max/1200/1*T6kVUKxG_Z4V5Fm1UXhEIw.png\"><img src=\"https://drive.google.com/uc?export=view&id=1QseZrLHjLba0mtSk4wfPbTLUd3oaQHQA\" width=400 heigth=400></a>\n",
        "\n",
        "This metric balances out precision and recall so we get a more stable estimate.\n"
      ],
      "metadata": {
        "id": "wxrI765qw2ke"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJnEIpZ94AkU"
      },
      "source": [
        "\n",
        "### ROC and AUC and Lift chart\n",
        "\n",
        "There are also two popular visualizations of error: [Receiver Operating Characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) and the connected metric [Area Under the Curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve) and [Lift charts](https://www3.nd.edu/~busiforc/handouts/DataMining/Lift%20Charts.html)\n",
        "\n",
        "\n",
        "**Ingo Mierswa, Founder of [RapidMiner](https://rapidminer.com/) gives a nice explanation:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-21T13:47:48.003046Z",
          "start_time": "2019-08-21T13:47:47.995371Z"
        },
        "hideCode": true,
        "hidePrompt": true,
        "id": "Ek6b3ojn4AkV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "outputId": "9d439a2e-70e0-42cf-cb48-ec5ba7e82abd"
      },
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "#HTML(\"<video width='600' height='600' controls> <source src='https://dms.licdn.com/playback/C4D05AQGEXYIQzbk2KA/6224596cd8cc43d7b45a2fd06245f858/feedshare-mp4_3300-captions-thumbnails/1507940147251-drlcss?e=1534773600&v=beta&t=2anCpPe4Kx7_62mxJ1PnmajWWtJUpMwj066sQhMey-E' type='video/mp4'></video>\")\n",
        "HTML(\"<video width='600' height='600' controls> <source src='http://drive.google.com/uc?export=view&id=1TO-W9Z982APWeeNL-tVdsHBJzTpYfznS' type='video/mp4'></video>\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<video width='600' height='600' controls> <source src='http://drive.google.com/uc?export=view&id=1TO-W9Z982APWeeNL-tVdsHBJzTpYfznS' type='video/mp4'></video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main concept of ROC and area under the ROC curve is about separability / \"separation\" of the classifier. \n",
        "\n",
        "A good description can be found [here](https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5).\n",
        "\n",
        "The ROC curve is basically plotting the **True Positive Rate** against the **False Positive Rate**.\n",
        "\n",
        "<a href=\"https://miro.medium.com/max/533/1*HgxNKuUwXk9JHYBCt_KZNw.png\"><img src=\"https://drive.google.com/uc?export=view&id=1Rvp-O95T8-zTJmCr_WUN_x1jpYGNVMT8\"></a>\n",
        "<a href=\"https://miro.medium.com/max/368/1*3GhDfiuhvINF5-9eL8g6Pw.png\"><img src=\"https://drive.google.com/uc?export=view&id=1al-eAYnaBq9bHJt6xQ0aWSKsRS2tJGXO\"></a>\n"
      ],
      "metadata": {
        "id": "Z-4PoLnc0i2G"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPUFJAMU4AkV"
      },
      "source": [
        "\n",
        "In case of complete separation the components look like this:\n",
        "\n",
        "<a href=\"https://miro.medium.com/max/792/1*Uu-t4pOotRQFoyrfqEvIEg.png\"><img src=\"https://drive.google.com/uc?export=view&id=1hFQLxuTNqdK2sFUmItCgOGOwZL0wwM1k\"></a>\n",
        "\n",
        "Which leads to the following ROC curve:\n",
        "\n",
        "<a href=\"https://miro.medium.com/max/548/1*HmVIhSKznoW8tFsCLeQjRw.png\"><img src=\"https://drive.google.com/uc?export=view&id=1KUVH61NWw-87r6TTm20ZRvQhpqkztJpc\"></a>\n",
        "\n",
        "In a not so perfect case it looks like this: \n",
        "\n",
        "<a href=\"https://miro.medium.com/max/761/1*yF8hvKR9eNfqqej2JnVKzg.png\"><img src=\"https://drive.google.com/uc?export=view&id=1eyQGruUfWfgrVzHoJFsgcBJdDXMuyZ6z\"></a>\n",
        "<a href=\"https://miro.medium.com/max/548/1*-tPXUvvNIZDbqXP0qqYNuQ.png\"><img src=\"https://drive.google.com/uc?export=view&id=1X-SCHumHKawWMPme4SDkZNbM-Cxe5nUs\"></a>\n",
        "\n",
        "And finally, if there is no separation at all, the curve looks like this:\n",
        "\n",
        "<a href=\"https://miro.medium.com/max/645/1*iLW_BrJZRI0UZSflfMrmZQ.png\"><img src=\"https://drive.google.com/uc?export=view&id=1A46tZ5x8Cgb348QRv_6hBoozjat2ras6\"></a>\n",
        "<a href=\"https://miro.medium.com/max/548/1*k_MPO2Q9bLNH9k4Wlk6v_g.png\"><img src=\"https://drive.google.com/uc?export=view&id=1B2Zn1e7WYIOaF6vhdJyviOngm7tKmdnK\"></a>\n",
        "\n",
        "[(source)](https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above-chance and worse-than-random models' ROC curves:\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/2000/1*qW3Mobeew1xxnXJnBPy8LQ.jpeg\" width=\"50%\">"
      ],
      "metadata": {
        "id": "5eCtIL5t0Qsw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generalization to more than two classes\n",
        "\n",
        "We often face non-binary casification problems, that is we have class labels drawn from a set (presented either as labels themselves or in a one-hot encoded form) so we have to extend our models to accomodate this.\n",
        "\n",
        "There are generally two strategies for this:"
      ],
      "metadata": {
        "id": "1ZhTly6m0_v1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Reduction to binary:\n",
        "\n",
        "We can decompose the multiclass classification problems into a combination of binary classification tasks in a few ways:\n",
        "\n",
        "#### One-vs-Rest\n",
        "\n",
        "We train $k$ binary classifiers (where $k$ is the number of classes), and take their output. We can choose the \"**most confident**\" of them as the class label (or use their output as combinations of class probabilities - see \"softmax\").\n",
        "\n",
        "#### One-vs-One\n",
        "\n",
        "We train ${k \\choose 2} = \\frac{k(k-1)}{2}$ binary classifiers and present them always with **data from two classes** which the classifiers have to learn to discriminate from each-other (this idea comes back in case of \"siamese networks\").\n",
        "\n",
        "#### Binary tree of binary classifiers\n",
        "\n",
        "In certain situations it can be advantageous to reduce the number of binary classifiers that need to be run during a prediction. One way of doing this is to treat the classification task as a decision problem, arrange the **classes into a binary tree** and train a **binary classifier for each decision point**. \n",
        "\n",
        "E.g., for 5 classes $C_1,\\dots,C_5$ there will be  classifiers deciding between classes $C_1 \\cup C_2 \\cup C_3$ vs $C_4 \\cup C_5$, $C_1 \\cup C_2$ vs $C_3$, $C_1$ vs $C_2$ etc. This construction requires $k-1$ classifiers for $k$ classes and ensures that at most $\\lceil \\log_2 k \\rceil$ classifiers are used for a single prediction."
      ],
      "metadata": {
        "id": "J6bWSjLb1EKV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. \"Natural extension\":\n",
        "\n",
        "For many models (like kNN, decision trees) there exists a natural extension to multi-class problems, since the model architecture is **naturally capable of assigning arbitrary class labels**, not just a binary decision.\n",
        "\n",
        "For **linear models** (and later on **neural networks**) some **special extension** of their output is necessary.\n"
      ],
      "metadata": {
        "id": "38WzH5Iy1G4l"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blhBkYmr4AkV"
      },
      "source": [
        "\n",
        "## Final note: Classification of anomalies\n",
        "\n",
        "If we understand that detection of anomaly / normal cases can be considered to be a **binary classification**, we acknowledge that in case of labeled data, we have a trivial way to use classifiers for anomaly detection. \n",
        "\n",
        "None the less there are extensions of classifiers (most notably [isolation forests](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html) and [one class SVM-s](http://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html), see more [here](http://scikit-learn.org/stable/modules/outlier_detection.html)) which are usable in an unsipervised learning case to detect anomalies."
      ]
    }
  ]
}