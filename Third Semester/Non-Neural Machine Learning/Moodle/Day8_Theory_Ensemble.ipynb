{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"ensambles\"></a>\n",
        "\n",
        "\n",
        "# Ensemble methods "
      ],
      "metadata": {
        "id": "efKcCmU6c__K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Motivation\n",
        "\n",
        "Motto: \"No one is as clever as everyone together.\"\n",
        "\n",
        "(A short presentation of the \"why\"-s of ensemble methods can be found [here](https://web.archive.org/web/20160803235557/http://www3.nd.edu:80/~rjohns15/cse40647.sp14/www/content/lectures/31%20-%20Decision%20Tree%20Ensembles.pdf).)\n"
      ],
      "metadata": {
        "id": "bTUkS2urdChJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Wisdom of crowds\n"
      ],
      "metadata": {
        "id": "nh-EPaHldEfn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Galton and the ox\n",
        "\n",
        "In his book [\"The wisdom of crowds\"](https://en.wikipedia.org/wiki/The_Wisdom_of_Crowds) Surowiecki tells the [anecdote of Galton](https://wisdomofcrowds.blogspot.com/2009/12/introduction-part-i.html) with the ox weighting competition, in which a **diverse set of people on average** guessed the weight of an ox significantly better than the best individual experts.\n",
        "\n",
        "(This contradicted Galton's elitist and \"eugenist\" view about the justified rule of \"the experts\".) \n",
        "\n",
        "Though itself a work about social decision making, we can learn a lot from this book, especially since the \"guessing competition\" is not that bad of a description of our machine learning tasks either.\n",
        "After surveying a broad range of situations, Surowiecki came up with [four requirements](https://en.wikipedia.org/wiki/The_Wisdom_of_Crowds#Four_elements_required_to_form_a_wise_crowd) that make a \"crowd wise\":\n",
        "\n",
        "| Criteria   |      Description      |\n",
        "|:---------- |:------------- |\n",
        "|_Diversity of opinion_|Each person should have private information even if it's just an eccentric interpretation of the known facts.|\n",
        "|_Independence_|People's opinions aren't determined by the opinions of those around them. (see eg. [groupthink](https://en.wikipedia.org/wiki/Groupthink))|\n",
        "|_Decentralization_|People are able to specialize and draw on local knowledge.|\n",
        "|_Aggregation_|Some mechanism exists for turning private judgments into a collective decision.|\n",
        "\n",
        "In the ox weighting case, people were obviously diverse, they did not discuss their opinions, had their own backgrounds and there was an averaging - done by Galton.\n",
        "\n",
        "(It is worth noting that these principles found practical use in some decision methodologies as eg. [decision markets](https://en.wikipedia.org/wiki/Prediction_market) or the [Delphi procedure](https://en.wikipedia.org/wiki/Delphi_method).)\n",
        "\n",
        "These principles will also be important in case of ensembles of machine learning classifiers. \n",
        "\n",
        "Takeaway: **We have to enforce the [independence assumption](http://www.matterofstats.com/mafl-stats-journal/2011/1/21/ensemble-models-for-predicting-binary-events.html) for ensembles.**\n"
      ],
      "metadata": {
        "id": "ixR3_NOXfBr2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Condorcet's jury theorem\n",
        "\n",
        "If we would like to have some clue about how \"bad\" the estimates can get before adding them starts to hurt the crowd, we can look into [Condorcet's jury theorem](https://en.wikipedia.org/wiki/Condorcet%27s_jury_theorem). It states that:\n",
        "\n",
        "\"The assumptions of the simplest version of the theorem are that a group wishes to reach a _decision by majority vote_. One of the two outcomes of the vote is correct, and each voter has an independent probability $p$ of voting for the correct decision. The theorem asks how many voters we should include in the group. The result depends on whether $p$ is greater than or less than 1/2:\n",
        "\n",
        "- If $p$ is greater than 1/2 (each voter is more likely to vote correctly), then adding more voters increases the probability that the majority decision is correct. In the limit, the probability that the majority votes correctly approaches 1 as the number of voters increases.\n",
        "- On the other hand, if $p$ is less than 1/2 (each voter is more likely to vote incorrectly), then adding more voters makes things worse: the optimal jury consists of a single voter.\"\n",
        "\n",
        "For us, it has the surprising conclusion that even when the \"experts\" are only having \"opinions\" **slightly better than chance, aggregating them can lead to improvement in performance**!\n",
        "\n",
        "_\"Weak learners unite!\"_"
      ],
      "metadata": {
        "id": "vGFWfqPyfHdl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Occam vs. Epicurus\n",
        "\n",
        "If adding more learners can in some cases be a good idea, we might also start to think differently about the fact that it is necessary to have the simplest model possible for a given phenomena.\n",
        "\n",
        "Philosophically we are accustomed to relying on \"Occam's razor\", by which we throw away more complex explanations in favor of an equally expressive simpler one. Much of statistical learning theory (like [VC complexity](https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension)) relies on this notion to prevent overfitting. \n",
        "\n",
        "Without formally refuting any of the notions of statistical learning theory, the case of ensembles might point towards the fact that maybe adding more \"learners\" (under circumstances) can be of benefit. In philosophical terms some [trace back](https://statmodeling.stat.columbia.edu/2009/07/27/dividing-netflix-prize-bayesian-philosophy/) this notion to Epicurus’ letter to Herodotus: “When, therefore, we investigate the causes of … phenomena, … we must take into account the variety of ways in which analogous occurrences happen within our experience.” ([source](http://www.epicurus.net/en/herodotus.html))\n",
        "\n",
        "In short: **It might be that overparametrization is not a fatal problem.** \n"
      ],
      "metadata": {
        "id": "V8c0l2lmftGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Sampling from a model distribution\n",
        "\n",
        "We have to consider the fact that for a given dataset even from the class of linear models, there exists an infinite amount that separates the data equally \"well\" (measured in a purely error, eg. misclassification based metric).\n",
        "\n",
        "<a href=\"https://cdn-images-1.medium.com/max/2000/1*UGsHP6GeQmLBeteRz80OPw.png\"><img src=\"https://drive.google.com/uc?export=view&id=13N_BO1AtlBZCLIpL3o5Yv57TI-nnQEvh\" width=55%></a>\n",
        "\n",
        "We can think of the data as something that \"restricts\" the set of \"plausible\" models:\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1UQK1IvY7mjcg_sODGA4x58cMh-A5jHyC\"><img src=\"https://drive.google.com/uc?export=view&id=1DDSzeKXUmTHNn7XSquZOsGWRcMur6Zzm\" width=65%></a>\n",
        "\n",
        "(This topic is also in strong connection with **overfitting** and **large-margin classifiers**.)\n",
        "\n",
        "From this view, we can try to think about the \"remaining\" infinite amount as a **distribution of models**, and thus it would absolutely make sense to do **unbiased sampling** from the model class, and to take the \"mean\" of the sample.\n",
        "\n",
        "We can argue that we do something like this with ensemble models (though we might violate independence assumptions). "
      ],
      "metadata": {
        "id": "hgCehbTfgIOA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Ensuring independence / diversity\n",
        "\n",
        "Since we saw that one of the key aspects of ensembles is the independence of the base models, we have to think about ways to ensure this. One of the approaches is to **let the models train on subsets of the data**, that is: we use **built-in crossvalidation** inside the ensemble fitting methods.\n",
        "\n",
        "This approach of intelligently subsampling the data and trying to ensure model robustness with it was already present in case of RANSAC and its [many variants](https://www.inf.ethz.ch/personal/pomarc/pubs/RaguramECCV08.pdf). \n",
        "\n",
        "More on ways for ensuring diversity [here](https://tel.archives-ouvertes.fr/tel-01662444/document) (p11)."
      ],
      "metadata": {
        "id": "AuPDfKgUgomk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiC_X3qbZuwZ"
      },
      "source": [
        "## Goals and types\n",
        "\n",
        "Ensemble methods are based on multiple (potentially _many_, potentially _individually weak_) **models grouped together into an \"ensemble\"** to aggregate their performance, thus gain overall prediction quality.\n",
        "\n",
        "A good overview of methods can be found in the blogpost [Ensemble Learning to Improve Machine Learning Results](https://blog.statsbot.co/ensemble-learning-d1dcd548e936).\n",
        "\n",
        "\"Ensemble methods are **meta-algorithms** that combine several machine learning techniques into one predictive model in order to:\n",
        "\n",
        "- decrease variance (bagging), \n",
        "- decrease bias (boosting), \n",
        "- or improve predictions (stacking).\n",
        "\n",
        "Ensemble methods can be divided into two groups:\n",
        "\n",
        "- **_sequential ensemble methods_**: where the base learners are generated sequentially (e.g. AdaBoost).The basic motivation of sequential methods is to exploit the dependence between the base learners. The overall performance can be boosted by weighing previously mislabeled examples with higher weight.\n",
        "- **_parallel ensemble methods_**: where the base learners are generated in parallel (e.g. Random Forest). The basic motivation of parallel methods is to exploit independence between the base learners since the error can be reduced dramatically by averaging.\n",
        "\n",
        "Most ensemble methods use a **single base learning algorithm** to produce homogeneous base learners, i.e. learners of the same type, leading to homogeneous ensembles.\n",
        "\n",
        "There are also some methods that use **heterogeneous learners**, i.e. learners of different types, leading to heterogeneous ensembles. In order for ensemble methods to be more accurate than any of its individual members, the base learners have to be as accurate as possible and as diverse as possible.\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sAAces8Zuwa"
      },
      "source": [
        "## Bagging (Bootstrap aggregation)\n",
        "\n",
        "<a href=\"https://www.kdnuggets.com/wp-content/uploads/bagging.jpg\"><img src=\"https://drive.google.com/uc?export=view&id=1uInTqYFnsv1wvQ0BeBkM1HwORbflOtRQ\" height=350px></a>\n",
        "\n",
        "- Goal is to decrease variance\n",
        "- Trains models in parallel, capitalizes on their independence\n",
        "- Bootstrap: using a subsample of a dataset (with replacement) -> strengthens independence\n",
        "- Variance decreases because of averaging (approximation)\n",
        "\n",
        "\n",
        "A good simple illustration can be found [here](https://machinelearningmastery.com/implement-bagging-scratch-python/)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random Forest\n",
        "\n",
        "<a href=\"https://www.researchgate.net/profile/Mariana_Recamonde-Mendoza/publication/280533599/figure/fig5/AS:267770621329410@1440852899493/Random-forest-model-Example-of-training-and-classification-processes-using-random.png\"><img src=\"https://drive.google.com/uc?export=view&id=1g8eiMr4ZunjyipHWD6VTs6EQ8kHzqUNc\" width=65%></a>"
      ],
      "metadata": {
        "id": "K10glYdgkI-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random forest algorithm\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=14TV7kY7Y1cKbsLLTW_9i5WEwdKOc4RDl\"><img src=\"https://drive.google.com/uc?export=view&id=1oqkjd4F_fF7kWXNwz457uXzV17mDp7Ti\" width=55%></a>\n"
      ],
      "metadata": {
        "id": "yPkfgICukL8D"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRx-vLzEZuwa"
      },
      "source": [
        "#### Effects\n",
        "\n",
        "It **decreases variance** by training decision trees in a randomized manner\n",
        "- All trees learn on a subset of data samples (subset selection with replacement, **bootstrap sample**)\n",
        "- All trees **use only a part of the variables**\n",
        "\n",
        "Pro:\n",
        "\n",
        " - Parallelizable\n",
        " - Pretty high performance\n",
        " - Helps interpretation (relevance of input variables can be gauged)\n",
        " - Drastically decreases variance\n",
        " - Less need for validation, has \"built in crossvalidation\"\n",
        " \n",
        "Con:\n",
        " - Adds a little bias\n",
        " - **Can only predict in the known range of values!**\n",
        "\n",
        "Detailed description [here](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#workings).\n",
        "\n",
        "\n",
        "It is worth noting that RF has a classification as well as a regression variant, so it is quite universal.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Boosting\n",
        "\n",
        "<a href=\"https://codesachin.files.wordpress.com/2016/03/boosting_new_fit5.png\"><img src=\"https://drive.google.com/uc?export=view&id=1nqG8dQwOO6oBsEKHwlA5UmGUzEpC6xtS\" width=50%></a>\n",
        "\n",
        "- Collects weak models and converts them to a strong one\n",
        "- We fit a **series of weak learners** for a **weighted set of teaching samples**\n",
        "- Iterative procedure, **\"hard cases\" in the data get more and more weight** during training\n",
        "- Weighted sum of created models is used on inference time\n",
        "- Not the same as bagging, since we **iteratively** train models and **on the whole of the data** (with weighting).\n"
      ],
      "metadata": {
        "id": "oYoHY3FjkmZ4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVKbZpkrZuwa"
      },
      "source": [
        "\n",
        "Illustration:\n",
        "\n",
        "<a href=\"https://www.researchgate.net/profile/Maria_Peraita-Adrados/publication/326379229/figure/fig5/AS:647978477948928@1531501516288/A-simple-example-of-visualizing-gradient-boosting.png\"><img src=\"https://drive.google.com/uc?export=view&id=1gg8130DbV6sx59pqcqfnUCRXkA2fYb31\" width=55%></a>\n",
        "\n",
        "Additional source [here](https://en.wikipedia.org/wiki/Boosting_(machine_learning))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example: AdaBoost\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1AZgJtem_hK22i0RhRb1fyp4nYIEOAD84\" width=35%>\n",
        "\n",
        "The AdaBoost algorithm is the application of the principle of boosting to a set of trees, thus can be considered a parallel to Random Forest, with the notable exceptions:\n",
        "\n",
        "1. AdaBoost is typically fitting __very shallow trees__, in fact, \"trees\" with only one decision (so called \"stumps\") whereby Random Forest does not restrict the trees so severely\n",
        "2. AdaBoost aggregates the final votes of individual constituents with a __weighting scheme__ during prediction, so while Random Forest gives equal vote to all resulting trees, AdaBoost explicitly gives higher say to models that were in a sense more \"right\"\n",
        "3. While Random Forest is a parallel algorithm (as bagging implies), AdaBoost is a __sequential__ one, so the fitting of a given tree is dependent on the previous fitted ones. \n",
        "\n",
        "Source of this description is the nice detailed introduction to Adaboost that can be found at [StatQuest](https://www.youtube.com/watch?v=LsK-xG1cLYA).\n"
      ],
      "metadata": {
        "id": "MMHCKCmwk83A"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-07-31T12:58:01.915923Z",
          "start_time": "2021-07-31T12:58:01.905649Z"
        },
        "id": "WkakTJrfchHa"
      },
      "source": [
        "\n",
        "#### Algorithmic steps:\n",
        "\n",
        "1. We initialize a vector for storing __sample weights__, initially as being of equal magnitude and __summing to one__.\n",
        "2. We __fit stumps__ on the individual data columns, calculate their __Gini index__ (as in decision trees), and choose the one with the __lowest Gini__ as the first chosen model.\n",
        "3. We calculate the __\"final say\"__ (the vote weight factor) of this stump. The \"say\" is based on the __total error__, that is the summed sample weight of the examples the stump did not get right. The Final Say is then given by the formula:\n",
        "$$ Final Say = {1\\over2}*{log({1- Total Error})\\over{Total Error}}$$\n",
        "In practice, the formula will generate a weighting like:\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1s_OVk7vYF8fAxMVARSw4nqgfCA6guDKc\" width=35%>\n",
        "\n",
        "\n",
        ">  So in the final voting \n",
        ">  * the \"clever\" models will get larger say, \n",
        ">  * the \"random guesser\" models no weight at all, \n",
        ">  * and the consistently opposite guesser \"anti-clever\" models will get a negative say, so their predictions will be inverted.\n",
        "\n",
        "\n",
        "4. Based on the previous stump, we now __recalculate the sample weights__ for the individual data points, so as to make the next stump fit more on the ones that were misclassified previously (thus can be considered more difficult). This we do as:\n",
        "$$new weight = \\begin{cases}\n",
        "  sample weight*e^{final say}, & \\text{if } misclassified, \\\\\n",
        "  sample weight*e^{-final say}, & \\text{otherwise}.\n",
        "\\end{cases}$$\n",
        "This will effectively result in a higher up-weighting for the misclassified examples and a more severe down-weighting of the correctly classified examples if the previous stump was more \"clever\", and a more lenient modification it it was \"dumber\". We finally __normalize the sample weights__ again to sum to one.\n",
        "\n",
        "5. We then go on to fit the new stump either by __weighted Gini indexes__ or just taking sample weights as probabilities and __oversampling the difficult data points__, creating a new dataset for the fit with sample weight set back to proportionate original values.\n",
        "6. We then __iteratively go on to fit__ newer and newer stumps, until either pre-set stump budget runs out or we get 0 error.\n",
        "7. At prediction time, we let all the stumps predict, __summarize their individual \"final says\", and take the majority vote__ accordingly."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reflection: not all datapoints are created equal\n",
        "\n",
        "It is interesting to note that the usage of \"informative\" examples can drastically reduce the needed amount of data for a classifier to learn, that is to say: **not all datapoints contribute equally to the performance** of a classifier.\n",
        "\n",
        "In a recent publication submitted to ICLR 2019. [An Empirical Study of Example Forgetting During Deep Neural Network Learning](https://openreview.net/pdf?id=BJlxm30cKm) by Toneva et al. it is quite obvious that there are in fact not so important datapoints (which the network never \"forgets\", misclassifies during the learning run). In fact, a shocking amount of data from this type can be removed without having much of an effect on the learning procedure!\n",
        "\n",
        "<a href=\"https://crazyoscarchang.github.io/images/2019-02-16-seven-myths-in-machine-learning-research/myth_4_2.png\"><img src=\"https://drive.google.com/uc?export=view&id=1nFsBMfmw134ecQwxjtsHscxqH-tSrVuk\" width=\"800px\"></a>\n",
        "\n",
        "\"Shockingly, 30% of the datapoints in CIFAR-10 can be removed, without changing test accuracy by much.\"\n",
        "\n",
        "[source](https://crazyoscarchang.github.io/2019/02/16/seven-myths-in-machine-learning-research/#myth-4)\n",
        "\n",
        "Maybe we have hope for small, not just big data?"
      ],
      "metadata": {
        "id": "etAvrwOkmtX_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Often - though not always - the informativeness of a datapoint is again in connection with the decision \"margin\". Informative points can be \"support vectors\". (See discussion on \"support vector machines\".)\n",
        "\n",
        "<a href=\"https://images.slideplayer.com/15/4793236/slides/slide_9.jpg\"><img src=\"https://drive.google.com/uc?export=view&id=10bmtQKa9c1CBQtEGh_Ayo0wYOO_ZWpg4\" width=55%></a>\n",
        "\n",
        "The concept of \"influential points\" also has connections to \"outliers\". Often times outliers have disproportionate influence (or even represent [\"label noise\"](https://www.semanticscholar.org/paper/A-comprehensive-introduction-to-label-noise-Fr%C3%A9nay-Kab%C3%A1n/c44f388832d6f309b1bb9ccdeddee491f195e6cd), that is, annotation errors), so it is not clear if extensive focus on these helps or hurts performance."
      ],
      "metadata": {
        "id": "I9GEy947nVRW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6o3yD4IOchHb"
      },
      "source": [
        "\n",
        "\n",
        "The [active learning](https://en.wikipedia.org/wiki/Active_learning_(machine_learning)) approach is capitalizing exactly on this effect: it asks targeted queries from an \"oracle\" (typically a human expert) so that it maximizes learning from the least amount of annotated data possible. \n",
        "\n",
        "<a href=\"https://cdn-images-1.medium.com/max/490/0*doMj6A96nyLxrzIU.png\"><img src=\"https://drive.google.com/uc?export=view&id=1bWC6q7FOQM4s1S-RCE0EUfxHFJUkM15G\" width=55%></a>\n",
        "\n",
        "There are even some methods that try to explicitly combine boosting with active learning, like [this](http://proceedings.mlr.press/v15/trapeznikov11a/trapeznikov11a.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1BwSRvRZuwa"
      },
      "source": [
        "### Gradient Boosting methods\n",
        "\n",
        "Generalization of boosting methods to any model with differentiable loss function.\n",
        "\n",
        "\"...two papers introduced the view of boosting algorithms as iterative functional gradient descent algorithms. That is, algorithms that optimize a cost function over function space by iteratively choosing a function (weak hypothesis) that points in the negative gradient direction. This functional gradient view of boosting has led to the development of boosting algorithms in many areas of machine learning and statistics beyond regression and classification.\"\n",
        "\n",
        "Sources: \n",
        "- [Wikipedia - Gradient boosting](https://en.wikipedia.org/wiki/Gradient_boosting)\n",
        "- [A Kaggle master explains gradient boosting](http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/) \n",
        "- [A gentle introduction to gradinet boosting](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/)\n",
        "- [Develop your first GB algorithm](https://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/)\n",
        "- [Xgboost - manual](http://xgboost.readthedocs.io/en/latest/)\n",
        "- [ML-Ensemble – high performance _custom_ ensemble learning library](https://github.com/flennerhag/mlens)\n",
        "\n",
        "Some widespread versions:\n",
        "* GBM\n",
        "* XGBoost\n",
        "* LightGBM\n",
        "* CatBoost\n",
        "\n",
        "**Takeaway 1:**\n",
        "It is to be noted that these methods are **in some cases even comparable with deep learning methods in performance**, so they are still absolutely relevant!\n",
        "\n",
        "**Takeaway 2:**\n",
        "**Gradient descent is a widely used method** for function approximation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stacking\n",
        "\n",
        "<a href=\"https://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier_files/stackingclassification_overview.png\"><img src=\"https://drive.google.com/uc?export=view&id=1XEaUudjSGs86jp6i0oUFyeeXJaDW2NwY\" width=\"500px\"></a>\n",
        "\n",
        "- Heterogenous, hierarchic method\n",
        "- We train different models on the whole available training data then use the output of these models as input to train a meta-model on top of them. **This approach will be highly relevant to deep learning!**"
      ],
      "metadata": {
        "id": "BvKAXhg8n8ql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional sources:\n",
        "- [Ensamble learning](https://web.archive.org/web/20210426174203/http://www.scholarpedia.org/article/Ensemble_learning)\n",
        "- [An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants](https://pdfs.semanticscholar.org/3ea3/a32db9315dc83175c225c300bf8e535bed0e.pdf)\n"
      ],
      "metadata": {
        "id": "od_6fKUhoNIe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Challenges of ensembles"
      ],
      "metadata": {
        "id": "g0uk8TODoVYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Which ensemble to use?\n",
        "\n",
        "([Source](https://tel.archives-ouvertes.fr/tel-01662444/document)) (p10-11)\n",
        "\n",
        "\"There are many ensemble classification methods with good performance. However,\n",
        "existing ensemble techniques have different drawbacks [209](https://dollar.biz.uiowa.edu/~street/research/baking_icdm.pdf).\n",
        "\n",
        "- Quinlan applied boosting and bagging to C4.5 [159](https://dl.acm.org/citation.cfm?id=152181) decision tree-based ensembles. Experimental results show that they can reduce the generalization error, and boosting has better effect than bagging. But in some cases, boosting can create overfitting [14](http://robotics.stanford.edu/~ronnyk/vote.pdf).\n",
        "- In boosting, each base classifier is trained on data that is weighted based on the performance of the previous classifier. The next base classifier focuses on the current samples which are classified with difficulty.\n",
        "- Boosting can not only reduce the bias but also reduce the variance [55](https://homes.cs.washington.edu/~pedrod/papers/aaai00.pdf), but bagging can only reduce the variance. Bagging does nothing purposely to reduce the bias so that any bias reduction is achieved solely by chance [21](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.115.7931&rep=rep1&type=pdf).\n",
        "- Boosting is sensitive to noise and outliers [14](http://robotics.stanford.edu/~ronnyk/vote.pdf). The noise sensitivity of AdaBoost is generally attributed to the exponential loss function which specifies that if an instance were not classified as the same as its given label, the weight of the instance will increase drastically. Consequently, when a training instance is associated with a wrong label, AdaBoost still tries to make the prediction resemble the given label, and thus degenerates the performance [212](https://web.archive.org/web/20181024101620/http://www2.islab.ntua.gr:80/attachments/article/86/Ensemble%20methods%20-%20Zhou.pdf).\n",
        "- Random forest combines Breiman’s \"bagging\" idea and the random selection of features. Randomness is introduced into the feature selection process [212](https://web.archive.org/web/20181024101620/http://www2.islab.ntua.gr:80/attachments/article/86/Ensemble%20methods%20-%20Zhou.pdf). Hence, random forest generates more diversity than bagging.\"\n",
        "\n",
        "**Short version: use RandomForest :-)**"
      ],
      "metadata": {
        "id": "xJ2Uot95oYvY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cautionary tale about deployability\n",
        "\n",
        "\"The Netflix Prize Challenge Competition ran for 34 months from October 2, 2006 until July 26, 2009. Netflix supplied a \"Training\" dataset of 100,480,507 ratings made by 480,189 Netflix clients for 17,770 movies between October, 1998 and December, 2005. The ratings were on a rating scale of one star to five stars. The Training data matrix has 99% missing data. Netflix also supplied a dataset of 2,817,131 \"Qualifying\" ratings. For these ratings, the clients and movies are known, but the actual ratings were known only to Netflix (until the competition concluded). The Netflix Prize was awarded to the team most successful at \"predicting\" those publicly unknown ratings. Teams were allowed to submit multiple prediction datasets.\n",
        "\n",
        "Team \"BellKor's Pragmatic Chaos\" won the Prize in a tie-break based on a 20-minute earlier submission time of their winning prediction dataset. The runners-up were team \"The Ensemble\" (of which I was a member).\"\n",
        "\n",
        "[source](https://www.rasch.org/rmt/rmt233d.htm)\n",
        "\n",
        "For the time, this was considered a huge dataset and a massive amount of teams were competing. As seen even in the names, ensembles were prominent approaches to the problem.\n",
        "\n",
        "Though the final prize was awarded (with some compromise), the real surprise happened when the engineering team of Netflix tried to come up with a way to integrate the winning solution into their systems. The end result of investigation was: **though accuracy is great, it is such a big ensemble of diverse models, that in practice, it won't work on the required scale**, thus the model was abandoned.\n",
        "\n",
        "(Wired did a nice [report](https://www.wired.com/2012/04/netflix-prize-costs/) on this.)\n",
        "\n",
        "Though it is _absolutely not_ true for \"standard\" ensemble models, like random forests or boosted trees, since they are fairly nimble and efficient in comparison for example to deep learning based models, care has to be taken that \"throwing everything you've got on the problem\" type of approaches can mean prohibitive engineering complexity, thus will prevent deploying the models in practice.  "
      ],
      "metadata": {
        "id": "lji-6N7joxf1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpretability\n",
        "\n",
        "At first glance, interpretability of huge ensemble models seems problematic. It is not immediately obvious how to understand and visualize the decision making process of eg. a large number of trees.\n",
        "\n",
        "Turns out that with quite simple methods one can obtain information about the importance of individual features in the tree (eg. by counting how frequently they are used as decision criteria)\n",
        "\n",
        "<a href=\"https://cdn-images-1.medium.com/max/1400/0*PA5LBCTjpevA2kZn\"><img src=\"https://drive.google.com/uc?export=view&id=1lt2kv7xBCJPgeat14KxLVSKDiNyMrPFw\" width=45%></a>"
      ],
      "metadata": {
        "id": "l_8XNw-7pbjN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also get a good idea of the forest through the visualization of just a few decision trees:\n",
        "\n",
        "<a href=\"https://cdn-images-1.medium.com/max/1400/0*41GThI3k1--f8PrV\"><img src=\"https://drive.google.com/uc?export=view&id=1oHhE_But9WxGXXc1ZYqCatu8KXQ5XUyi\" width=45%></a>\n",
        "\n",
        "More on visual analysis of random forests can be found [here](https://medium.com/@garg.mohit851/random-forest-visualization-3f76cdf6456f) and [here](https://towardsdatascience.com/interpreting-random-forest-and-other-black-box-models-like-xgboost-80f9cc4a3c38).\n"
      ],
      "metadata": {
        "id": "VtnochagppFz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thus, we can conclude that homogeneous ensembles are quite interpretable. This is not necessarily true for heterogeneous ones.\n",
        "\n",
        "(It is interesting to note that much of the interpretability relies on the fact that the inputs are themselves having meaning, and since no hierarchic transformation is present, we can rely directly on this meaning.)\n",
        "\n"
      ],
      "metadata": {
        "id": "ANosPp91prTL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## \"Staying in the original data space\"\n",
        "\n",
        "One limitation of even the most sophisticated **tree methods** is that they can learn to compartmentalize the data in really complex ways, but **do not learn any systematic, function like transformation** of the data space. \n",
        "\n",
        "For a better illustration, let's take a look at the decision surfaces of ensembles!"
      ],
      "metadata": {
        "id": "oZhsIjvpqMDm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Decision surfaces of ensembles\n",
        "\n",
        "One of the main properties of a model is the shape of the decision boundary it can represent. In this regard, ensembles have some distinctive advantages.\n",
        "\n",
        "Even in case of an ensemble of classifiers with low degrees of freedom (\"simple classifiers\") it is true that **the final decision boundary of the ensemble can become non-trivially shaped**:\n",
        "\n",
        "<a href=\"https://images.slideplayer.com/25/8236676/slides/slide_4.jpg\"><img src=\"https://drive.google.com/uc?export=view&id=1l6GyzMTXPy4T4YCYmvtQpjjKEYTR2t3v\" width=65%></a>\n",
        "\n",
        "[Source](https://web.archive.org/web/20160803235557/http://www3.nd.edu:80/~rjohns15/cse40647.sp14/www/content/lectures/31%20-%20Decision%20Tree%20Ensembles.pdf)"
      ],
      "metadata": {
        "id": "qHgVJN51qWzd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In case of tree based models the process can look somewhat like this:\n",
        "\n",
        "<a href=\"https://shapeofdata.files.wordpress.com/2013/07/randforest.png\"><img src=\"https://drive.google.com/uc?export=view&id=1we6TKDivvurpsxU86Ys29HKfCydY_aQe\" width=55%></a>\n",
        "\n",
        "And results in a highly non-trivial boundary eg. in case of a random forest model:\n",
        "\n",
        "<a href=\"https://i.stack.imgur.com/HXGXw.png\"><img src=\"https://drive.google.com/uc?export=view&id=1GpKNFgpJoJyvbGp3h7bq2McllU7Zr95i\" width=55%></a>\n",
        "\n",
        "Takeaway: **ensemble methods can \"puzzle together\" complex shapes for decision surfaces**"
      ],
      "metadata": {
        "id": "hHz4d1gqqfCz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is to be concluded that the ability of these models to construct complex decision boundaries in a \"local\" \"piecewise\" manner is of great advantage - especially if the built in crossvalidation approaches prevent it from overfitting.\n",
        "\n",
        "We will come back to this ability of **\"piecewise construction\" of decision surfaces** in the case of deep learning later on.\n",
        "\n",
        "There is also some research on the behavior of \"ensemble margins\", e.g. [here](https://tel.archives-ouvertes.fr/tel-01662444/document) (p12-p15) and [here](http://www.cs.man.ac.uk/~stapenr5/publications/tech_report2012.pdf) which tries to investigate if there is any connection between ensemble decision boundaries and \"large margin\" methods."
      ],
      "metadata": {
        "id": "la_zGL9tqqOD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### No transformation\n",
        "\n",
        "Though the decision surfaces of these models are complex, **they still operate inside the topology of the original data distribution**. Deep learning methods will behave in a markedly different manner, the presence of successive (hierarchic) transformations of the data will be crucial.\n",
        "\n",
        "**Intuition:** what if we would not go for a complex surface, just transform (\"systematically rearrange\") the data, so that the same classes would get near to each-other, and then we would only need to learn a simple decision boundary on top?\n",
        "\n",
        "It is worth noting that this shortcoming of eg. random forests was attacked by a line of research proposing some kind of hierarchic structure for forests like in [this paper](https://arxiv.org/abs/1702.08835), or hybrid solutions like [here](https://www.cs.cmu.edu/~mfiterau/papers/2016/IJCAI_2016_DNDF.pdf) and [here](https://arxiv.org/abs/1807.06699).\n",
        "\n",
        "Stacking is in a sense also a rudimentary solution pointing in this direction.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XD8SDJ7Tqssb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgB1VAikZuwb"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "In _many_ problem domains, ensemble models, especially RandomForest and XGBoost are having **dominant performance**. In fact many of the open competitions at Kaggle.com are being won by these methods, and a dedicated paper titled  [Do we Need Hundreds of Classifiers to Solve Real World\n",
        "Classification Problems?](http://www.jmlr.org/papers/volume15/delgado14a/delgado14a.pdf) also concluded that RandomForest is a _very_ good guess to solve problems.\n",
        "\n",
        "None the less, the big advantage of **deep learning based methods** will shine in case of **huge dimensionality** of input data (think hundreds or thousands of features in a really complex shape) that can be disentangled by the right (learned) representations. But more on that later.\n",
        "\n",
        "Moreover: we can assume that for a generally strong \"learner\" there are some properties that define its learning ability. \"**Crossvalidation**\", \"**piecewise construction**\" of a high capacity (non linear) decision surface are important characteristics, and are _shared_ by ensemble methods and deep learning - if we analyze them deep enough. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "165px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}