{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Supervised Algorithms\n",
    "    * Flash cart\n",
    "2. Unsupervised Algorithms\n",
    "    * Category\n",
    "3. Reinforcement Algorithms\n",
    "    * reward and punishment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Generate some random data\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print the intercept and coefficients of the linear regression line\n",
    "print(\"Intercept:\", model.intercept_, \" Coefficient:\", model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, precision_score, accuracy_score, recall_score,f1_score\n",
    "\n",
    "# Compute metrics\n",
    "MSE = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "# CA = accuracy_score(y_test, y_pred)\n",
    "# Percision = precision_score(y_test, y_pred)\n",
    "# Recall = recall_score(y_test, y_pred)\n",
    "# F1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"MSE:\", MSE,\" R-squared:\", r2, \" Accuracy:\", CA, \" Precision:\", Percision, \" Recall:\", Recall, \" F-measure:\", F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the data and the linear regression line\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, model.predict(X), color='red')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate a random binary classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_informative=2,\n",
    "                            n_redundant=0, n_clusters_per_class=1,\n",
    "                            class_sep=1.0, random_state=42)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. SVM (Support Vector Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only use the first two features for visualization purposes\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train the SVM classifier\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"C:\", model.C, \" kernel:\", model.kernel, \" gamma:\", \n",
    "      model.gamma,\" degree:\", model.degree, \" coef0:\", model.coef0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, precision_score, accuracy_score, recall_score,f1_score\n",
    "\n",
    "# Compute metrics\n",
    "MSE = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "CA = accuracy_score(y_test, y_pred)\n",
    "Percision = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "Recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "F1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "r2 = round(r2,3)\n",
    "Percision = round(Percision,3)\n",
    "F1 = round(F1,3)\n",
    "\n",
    "print(\"MSE:\", MSE,\" R-squared:\", r2, \" Accuracy:\", CA, \" Precision:\", Percision, \" Recall:\", Recall, \" F-measure:\", F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot the decision boundary and the support vectors\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='viridis')\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = model.decision_function(xy).reshape(XX.shape)\n",
    "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n",
    "ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=100, linewidth=1, facecolors='none', edgecolors='k')\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. KNN (K-Nearest Neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples = 500, n_features = 2, centers = 4,cluster_std = 1.5, random_state = 4)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm: auto  Leaf Size: 30\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Train the KNN model\n",
    "model_1 = KNeighborsClassifier(n_neighbors=1)\n",
    "model_1.fit(X_train, y_train)\n",
    "\n",
    "model_5 = KNeighborsClassifier(n_neighbors=5)\n",
    "model_5.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_1 = model_1.predict(X_test)\n",
    "y_pred_5 = model_5.predict(X_test)\n",
    "\n",
    "print(\"Algorithm:\", model_1.algorithm, \" Leaf Size:\",model_1.leaf_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy_1 = accuracy_score(y_test, y_pred_1)\n",
    "accuracy_5 = accuracy_score(y_test, y_pred_5)\n",
    "\n",
    "print(\"Accuracy (1):\", accuracy_1,\"Accuracy (5):\", accuracy_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(X_test[:,0], X_test[:,1], c= y_pred_5, marker= '*', s= 100,edgecolors= 'black')\n",
    "plt.title(\"Predicted values with k=5\", fontsize= 20)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(X_test[:,0], X_test[:,1], c=y_pred_1, marker= '*', s=100,edgecolors='black')\n",
    "plt.title(\"Predicted values with k=1\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "# Generate a random binary classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_informative=2,\n",
    "                            n_redundant=0, n_clusters_per_class=1,\n",
    "                            class_sep=1.0, random_state=42)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# Train the decision tree classifier\n",
    "# max_features = auto\n",
    "model = DecisionTreeClassifier(max_depth= 2, random_state= 42, min_samples_split= 100, min_samples_leaf=2)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "tree = plot_tree(model, filled=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.bar(range(len(iris.feature_names)), rf.feature_importances_)\n",
    "plt.xticks(range(len(iris.feature_names)), iris.feature_names)\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Generate a random dataset with 3 clusters\n",
    "X, y = make_blobs(n_samples=300, centers=3, cluster_std=0.5, random_state=42)\n",
    "\n",
    "# Train the K-Means clustering model\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Predict the cluster labels for the dataset\n",
    "y_pred = kmeans.predict(X)\n",
    "\n",
    "# Plot the clusters and centroids\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, marker='*', c='red')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Extract the first two features for visualization\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Define a grid of points to visualize the decision regions\n",
    "xx, yy = np.meshgrid(np.arange(X[:, 0].min() - 1, X[:, 0].max() + 1, 0.1),\n",
    "                     np.arange(X[:, 1].min() - 1, X[:, 1].max() + 1, 0.1))\n",
    "Z = nb.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z[:, 1].reshape(xx.shape)\n",
    "\n",
    "# Plot the decision regions and the training data\n",
    "plt.contourf(xx, yy, Z, alpha=0.5)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\n",
    "plt.xlabel(iris.feature_names[0])\n",
    "plt.ylabel(iris.feature_names[1])\n",
    "plt.title(\"Naive Bayes Decision Regions\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Dimensional Reduction Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Perform PCA with two principal components\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
    "plt.xlabel('First principal component')\n",
    "plt.ylabel('Second principal component')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
