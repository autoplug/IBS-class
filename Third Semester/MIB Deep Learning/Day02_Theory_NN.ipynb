{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-udDLHGtPDE"
      },
      "source": [
        "<a id=\"nnbasics\"></a>\n",
        "# Neural network basics \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mxx1lK_Vgc4"
      },
      "source": [
        "## AI winter\n",
        "\n",
        "Worth reading: [Wikipedia - AI winter](https://en.wikipedia.org/wiki/AI_winter)\n",
        "\n",
        "<a href=\"https://image.slidesharecdn.com/20170311aipresentationtorecord-170329202319/95/a-primer-on-artificial-intelligence-ai-and-machine-learning-ml-8-638.jpg?cb=1490819184\"><img src=\"https://drive.google.com/uc?export=view&id=1Xu3eazocOHN0m34Acfx3CwjIzy9hkNGV\"></a>\n",
        "\n",
        "\"The AI industry, which many market researchers had projected would reach 4 billion USD in annual sales by now, remains nascent. Generous estimates of the market today are closer to 600 million USD. After swallowing up hundreds of millions of dollars in venture capital…hundreds of AI start-ups have yielded only a few profitable public companies. (Wall Street Journal, “Bright Outlook for Artificial Intelligence Yields to Slow Growth and Big Cutbacks”, 7/5/1990.)\"\n",
        "\n",
        "[Source](http://reactionwheel.net/2015/01/80s-vc.html)\n",
        "\n",
        "\"Expert systems: Human Issues MIT Press,1990 Despite almost two decades of intensive R&D, there are still relatively few expert systems in working practice. If a computer is providing “advice\" how far can that advice be trusted? Who is responsible if it turns out to be wrong? Do the decisions of human experts depend on implicit knowledge that cannot be captured ... Essays in this book consider case studies of a medical expert system and management of child abouse cases, issues in designing expert systems for business... a systemic approach to organizational... and responsibility issues. The editors conclude with a brief discussion of integrating expert systems with other computer systems... legal implications, the possibilities of joint cognitive systems (seeking to supplement human supplement rather than replace them), and worries about deskilling ALSO SEE: Bright Outlook for Artificial Intelligence (Wall Street Journal. 5 July 1990) on ** employment cuts in the AI industry ranging from 50-90%.** The biggest AI market is for expert systems; AI is also used in computerized vision system and speech recognition system.\"  - Future Survey Annual 1992\n",
        "\n",
        "## But what on earth are we talking about after all?\n",
        "\n",
        "<a href=\"https://image.slidesharecdn.com/tekesfuturewatchchinamodule26ai-170825094305/95/future-watch-chinas-digital-landscape-and-rising-disruptors-module-26-artificial-intelligence-7-638.jpg?cb=1503654352\"><img src=\"https://drive.google.com/uc?export=view&id=1XAKZjuotysr88CcuQQvuULTB_l19bljY\"></a>\n",
        "\n",
        "And:\n",
        "\n",
        "### Automatic speech recognition results:\n",
        "<a href=\"https://cdn-images-1.medium.com/max/1600/1*eRD00-2GaCjz97AfhHlQoQ.png\"><img src=\"https://drive.google.com/uc?export=view&id=1sFeopvD9X7N7jvpF8camcPIpOhu--C1z\"></a>\n",
        "\n",
        "### Where can I follow the developments?\n",
        "\n",
        "Nice summary page:\n",
        "\n",
        "### **[Electronic Frontier Foundation - AI progress metrics](https://www.eff.org/ai/metrics)**\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1pelDZmTING7dbgqborEW16Pr0PV7y0-_\"><img src=\"https://drive.google.com/uc?export=view&id=16BZMRiYDsz_Brvyz430nehkOoCkCMLd4\"></a>\n",
        "\n",
        "Or more up to date:\n",
        "\n",
        "[Papers with code](https://paperswithcode.com/state-of-the-art)\n",
        "\n",
        "\n",
        "## \"The resistance\"\n",
        "\n",
        "During the AI winters the mainstream of scientific community - in image recognition, speech, language and in other AI fields - was strongly against the usage of neural networks, since they regarded it as a \"dead paradigm\". But the history of AI as a scientific field teaches us not to disregard some old ideas, since they can easily arise in new forms again (like genetic algorithms entering mainstream again in [this](https://www.technologyreview.com/s/611568/evolutionary-algorithm-outperforms-deep-learning-machines-at-video-games/) case). \n",
        "\n",
        "The \"resistance\" during the neural network winter is well represented by the work of [**Geoffrey Hinton**](https://en.wikipedia.org/wiki/Geoffrey_Hinton), who was working on new learning algorithms for neural models in the mid 80s, especially backpropagation, thus securing his name as [\"the godfather of deep learning\"](https://www.youtube.com/watch?v=uAu3jQWaN6E) - and a media celebrity and \"face\" for the movement.)\n",
        "\n",
        "<a href=\"https://images.thestar.com/C_Dnyhg8tb3tVXiGtq93vee9oJM=/1200x799/smart/filters:cb(1524397170509)/https://www.thestar.com/content/dam/thestar/news/world/2015/04/17/how-a-toronto-professors-research-revolutionized-artificial-intelligence/geoffrey-hinton-3.jpg\"><img src=\"https://drive.google.com/uc?export=view&id=1fyGu0l-IRygH3kZI29obj_QICJrDTBZs\" width=600 heigth=600></a>\n",
        "\n",
        "Others also followed. (Except Schmidhuber, who claims: he started the whole thing on his own. :-)\n",
        "\n",
        "<a href=\"https://i.imgur.com/lq1LDVO.png\"><img src=\"https://drive.google.com/uc?export=view&id=1meHVdAid2oN1UPEHuUjXRG5NIrAYwGCm\" height=\"400\" width=\"400\" align=\"left\"></a>\n",
        "\n",
        "\n",
        "### Jürgen Schmidhuber,\n",
        "### Joshua Bengio, \n",
        "### Geoffrey Hinton,\n",
        "### Yann LeCun,\n",
        "### Andrew Ng"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFFWu9XhVgc9"
      },
      "source": [
        "## \"Multilinearity\"\n",
        "\n",
        "### We were here\n",
        "<a href=\"https://www.mathworks.com/help/nnet/ug/percept_percla.gif\"><img src=\"https://drive.google.com/uc?export=view&id=1N05roQQnKMenfDGTnBbsWf9g61rBUqLk\"></a>\n",
        "\n",
        "### Our problem was\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1m59mOWDu7yShgMpSgw2yC8YDVzi1-fzs\"><img src=\"https://drive.google.com/uc?export=view&id=1Ci6_UtG_Lr2UnwQQv6r86382TnSfGTf5\" style=\"width: 80%;\"></a>\n",
        "\n",
        "## Solution: we need more layers! \n",
        "\n",
        "Even Minsky and Papert suggested this solution to the XOR problem\n",
        "\n",
        "<a href=\"http://slideplayer.com/slide/778829/3/images/5/Minsky+&+Papert+(1969)+offered+solution+to+XOR+problem+by.jpg\"><img src=\"https://drive.google.com/uc?export=view&id=10nkEXJlBBCV3Wj-24ywMKCaMy0TJu46e\" width=600 heigth=600></a>\n",
        "\n",
        "### How does it look in practice?\n",
        "\n",
        "<a href=\"http://scikit-learn.org/stable/_images/multilayerperceptron_network.png\"><img src=\"https://drive.google.com/uc?export=view&id=1zKuJIjW-3lcySUNB4gIJqwCETctQBhij\" width=400 heigth=400></a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYzadGq7VgdA"
      },
      "source": [
        "## Smarter learning method\n",
        "\n",
        "### Problems with the perceptron learning algorithm\n",
        "\n",
        "#### How should we adapt the algorithm for more than one layers?\n",
        "\n",
        "It is unclear how we should compute the error and update the weights on the basis of the error -- we need an analytical method for computing the individual weights' contribution \"backward\" from the error.\n",
        "\n",
        "#### We have already seen the dangerous order sensitivity + it considers only one example at a time \n",
        "In practice the perceptron update rule is quite scary:\n",
        "\n",
        "<a href=\"https://jeremykun.files.wordpress.com/2011/08/perceptron-iterations.gif\"><img src=\"https://drive.google.com/uc?export=view&id=1-WNDl_ahHQp-cbpEQa5_Jy4hK9QYY47j\"></a>\n",
        "\n",
        "#### What if the data is not linearly separable?\n",
        "We have no guarantee that the resulting weights will be optimal, i.e., that the algorithm minimizes the number of errors. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmg4nRQ6VgdE"
      },
      "source": [
        "## Learning as a minimization task\n",
        "\n",
        "Basic intuition: we should try to find those model parameters that minimize how \"bad\" the model is. \"Badness\" in this context is not exclusively the error on our data set, a model can be worse than another because it's too complex, its weights are too large etc. The function to be minimized is typically called \"loss\" or \"objective\" function. \n",
        "\n",
        "### Error rate as loss function (0/1 loss)\n",
        "\n",
        "The error rate or error number on the training data might seem to be a good loss function but, suprisingly, there is no usable optimization algorithm for this type of loss, even for the linear case:\n",
        "\n",
        "- The problem of finding the linear separator with the minimal error rate/number of errors is NP-hard, i.e., there is a polynomial time algorithm only if P=NP.  Even more surprisingly, the problem of approximating the minimum is also NP-hard $\\Rightarrow$ it is advisable to experiment with different loss functions.\n",
        "\n",
        "- One of the main problems of the error rate functions is that in certain cases small changes in the parameters can lead to large jumps in the function value, while in others even large parameter values do not change the output. Solution: we work with smoothed loss functions that correlate with the error rate (e.g., being its upper limit) but which are easier to minimize.\n",
        "\n",
        "<a href=\"http://fa.bianp.net/blog/images/2014/loss_01.png\"><img src=\"https://drive.google.com/uc?export=view&id=1kZHdEi-yZQEn9FM45I0uDOqSeKfstNkL\"></a>\n",
        "\n",
        "<a href=\"http://fa.bianp.net/blog/images/2014/loss_log.png\"><img src=\"https://drive.google.com/uc?export=view&id=1uoVkubaxEwZgJV779tdlOj135FSOtDt-\"></a>\n",
        "\n",
        "### Surrogate loss functions\n",
        "\n",
        "<a href=\"http://i.imgur.com/r37lX2P.jpg\"><img src=\"https://drive.google.com/uc?export=view&id=18f0vny5HKfBa8W_6YYcqqNr1PfZJXvHm\"></a>\n",
        "\n",
        "\n",
        "It is important to note that in addition to the optimization problem, there are other important reasons for using a surrogate loss function instead of 0/1 loss: 0/1 loss is too sensitive to small perturbations, which means that optimizing the network for it would lead to overfitting in most of the cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GRegsIRVgdJ"
      },
      "source": [
        "## What is the solution? -- Gradient descent\"\n",
        "<a href=\"https://sebastianraschka.com/images/blog/2015/singlelayer_neural_networks_files/perceptron_animation.gif\"><img src=\"https://drive.google.com/uc?export=view&id=1IVQBJxq8tf0vsMWw9vVI_PutXJ0J5aL3\"></a>\n",
        "\n",
        "The Gradient Descent algorithm takes a small step in right direction taking into consideration all examples of the training data, so it moves toward the minimum in the error space.\n",
        "\n",
        "## Gradient Descent algorithm\n",
        "If the function to be minimized is the differentiable $F:\\mathbb R^n\\rightarrow \\mathbb R$,  $\\eta _n$ is a series of learning rates and  $K$ is the number of steps then the algorithm is\n",
        "\n",
        "1. $\\mathbf p_0 :=  \\mathbf p_{\\mathrm init}$ (initial parameters  [for neural nets these are the weights and biases]):\n",
        "2. for k $\\in [1..K]$:\n",
        "    - $\\mathbf g_k :=\\Delta F(\\mathbf p_{k-1})$ (compute the value of the derivative for the actual parameters)            \n",
        "    - $\\mathbf p_k := \\mathbf p_{k-1} - \\eta_k \\mathbf g_k$ (take a step in the direction of the gradient)\n",
        "3. The result is $\\mathbf p_K$.\n",
        "\n",
        "<a href=\"https://sebastianraschka.com/images/blog/2015/singlelayer_neural_networks_files/perceptron_gradient_descent_1.png\"><img src=\"https://drive.google.com/uc?export=view&id=16Ejy98onOnUqHZ6PbuK8Ewp_6_b7PwP6\"></a>\n",
        "\n",
        "### Convergence\n",
        "\n",
        "Under certain conditions (among others, $F$ has to be convex)  the series generated by the algorithm is guaranteed to converge to the minimum: the $\\eta_n$ series can be chosen to be a constant series such that the convergence rate is $\\mathcal O(1/k)$, that is,  if $F$ reaches its minimum at $\\mathbf p^*$ then there is an $\\alpha$ constant for which for any $k$, $F(\\mathbf{p}_k)-F(\\mathbf{p}^*)\\leq \\frac{\\alpha}{k}$.\n",
        "\n",
        "### Too small and too large learning rates\n",
        "\n",
        "<a href=\"https://cdn-images-1.medium.com/max/1600/1*EP8stDFdu_OxZFGimCZRtQ.jpeg\"><img src=\"https://drive.google.com/uc?export=view&id=1e0NZxneJuJyT6CRVDW7gmjZ7sLzpDW28\" width=\"600\"></a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlSfbVXQ0Dqy"
      },
      "source": [
        "## Moving from the parameter space to the \"error space\"\n",
        "\n",
        "The change of the decision boundary corresponds to decreasing the error.\n",
        "<a href=\"https://iamtrask.github.io/img/sgd_optimal.png\"><img src=\"https://drive.google.com/uc?export=view&id=1Yq73R3HMAz2afnw1TUcH4w0M15MKLYo2\"></a>\n",
        "<a href=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/23104835/Qc281.jpg\"><img src=\"https://drive.google.com/uc?export=view&id=1C5VsVjrzBmBMtLZjot4eo02ChUu0Q6qH\"></a>\n",
        "\n",
        "**Notice that if the separation is nonlinear then curvature of the decision boundary/surface increases. Somewhat simplifying the matter we can assume that the weight vector's values are increasing and the decision surface can be described by a higher order polynomial.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRPtY0voVgdL"
      },
      "source": [
        "## \"Nonlinearity\"\n",
        "\n",
        "- An early idea\n",
        "- Firstly the role was played by the\n",
        "\n",
        "**Sigmoid (logistic) function**\n",
        "\n",
        "$${\\displaystyle S(x)={\\frac {1}{1+e^{-x}}}={\\frac {e^{x}}{e^{x}+1}}.}$$\n",
        "\n",
        "<a href=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/600px-Logistic-curve.svg.png\"><img src=\"https://drive.google.com/uc?export=view&id=17iKxq3jA3ZTBt5PX7HSjg6bHXQswmPam\" width=400 heighth=400></a>\n",
        "\n",
        "### Its advantages\n",
        "\n",
        "- Differentiable (unlike the step function used in the perceptron)\n",
        "- It is the smoothed version of the unit step function\n",
        "  <a href=\"http://drive.google.com/uc?export=view&id=1fTTksMeUU9UTZLUDq4NF5avktkQ_p5IU\"><img src=\"https://drive.google.com/uc?export=view&id=1V3_6m8qRaD0oaddyrZfe3qf6O60P_lgm\"></a>\n",
        "- Its range is the  $(0,1)$ interval, so its values can be interpreted as probabilities\n",
        "- Can produce complex decision surfaces when used in multiple layers\n",
        "\n",
        "See also:\n",
        "\n",
        "<a href=\"https://sebastianraschka.com/images/faq/diff-perceptron-adaline-neuralnet/8.png\"><img src=\"https://drive.google.com/uc?export=view&id=1YyoucH1nwkG48T6EiZDoZHwc1SjWgT1X\"></a>\n",
        "\n",
        "(Adaline is a perceptron variant trained with a \"smarter learning rule\", see [here](https://sebastianraschka.com/faq/docs/diff-perceptron-adaline-neuralnet.html) ) \n",
        "\n",
        "<a href=\"https://i.stack.imgur.com/xcdwn.png\"><img src=\"https://drive.google.com/uc?export=view&id=1s8Kk4IztXBGTGHjWJHkyVy8DcueeL_eW\" width=60%></a>\n",
        "<a href=\"https://i.stack.imgur.com/blIBz.png\"><img src=\"https://drive.google.com/uc?export=view&id=1hxfNGUCMPo0QFPdsO0wQToTW8IoELFUh\" width=60%></a>\n",
        "<a href=\"https://ars.els-cdn.com/content/image/1-s2.0-S089360809700097X-gr3.gif\"><img src=\"https://drive.google.com/uc?export=view&id=1gz53JpAga2rjFW-MdPc3xeVTIx-v6uvo\" width=55%></a>\n",
        "\n",
        "[Source](https://stats.stackexchange.com/questions/291492/how-can-logistic-regression-produce-curves-that-arent-traditional-functions)\n",
        "\n",
        "\n",
        "Another important advantage is that it is a **universal approximator**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XG3f80dssKZJ"
      },
      "source": [
        "<a id=\"historyagain\"></a>\n",
        "# History revisited\n",
        "\n",
        "- During \"winter\" for neural models, some researchers worked - mainly in isolation - on problems regarding neural models\n",
        "- Results paved way for a later \"revolution\"\n",
        "\n",
        "- Amongst them [Georg Cybenko](https://en.wikipedia.org/wiki/George_Cybenko) \n",
        "- Proof of  \"universal approximaiton theorem\" (see below)\n",
        "- Together with the backpropagation algorithm applied by Hinton, gave new hope to research in multi layer neural networks\n",
        "\n",
        "<a href=\"https://c1.staticflickr.com/9/8568/15881996631_f9029074d5_b.jpg\"><img src=\"https://drive.google.com/uc?export=view&id=1E7U6lNg5cduF6OCr9aViszX29I0_Gh9t\" width=600 heigth=600></a>\n",
        "\n",
        "(After the theorem, Cybenko shifted his attention to other areas and problems of mathematics, and thus is not part of the \"deep learning movement\".)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlPLzKFJsQkN"
      },
      "source": [
        "## Universal approximation theorem (Cybenko, 1989)\n",
        "\n",
        "Let $F$ be a continuous function on a closed and bounded subset of $\\mathbb R^n$ and $\\sigma$ any sigmoid-like function, that is, $\\sigma:\\mathbb R\\rightarrow\\mathbb R$, continuous and $\\lim_{x \\to -\\infty} \\sigma(x) = 0$ and $\\lim_{x \\to \\infty} \\sigma(x) = 1$. Then for any $\\epsilon>0$ there exists a $\\hat{f}$ finite neural network with a single hidden layer whose activation function is $\\sigma$  and which is closer to $f$ than $\\epsilon$, that is,\n",
        "\n",
        "$$\\forall \\mathbf x \\in \\mathrm{Dom}(f): \\left|~f(\\mathbf x) - \\hat{f}(\\mathbf x)\\right|< \\epsilon$$\n",
        "\n",
        "Original paper: [here](https://www.dartmouth.edu/~gvc/Cybenko_MCSS.pdf). Somewhat clearer version of the original proof: [here](http://mcneela.github.io/machine_learning/2017/03/21/Universal-Approximation-Theorem.html).\n",
        "\n",
        "Illustration:\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1xIKYRXMchV6B6C9WmhwK0qCENbkIYGiv\"><img src=\"https://drive.google.com/uc?export=view&id=1OeKexw1HCSLxrMqubc3ruOtwnHmtXo5K\" style=\"width: 45%;\"></a>\n",
        "\n",
        "[Source](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.101.2647&rep=rep1&type=pdf)\n",
        "\n",
        "\n",
        "### Some generalizations\n",
        "\n",
        "- Hornik, 1990: The theorem also holds for continuous, bounded and not constant activation functions [here](https://web.archive.org/web/20190818232735/http://zmjones.com:80/static/statistical-learning/hornik-nn-1991.pdf)\n",
        "\n",
        "- Leshno et al, 1993: It is enough to require piecewise continuity and so called local boundedness if the function is not polynomial.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAcrhaFzsZc1"
      },
      "source": [
        "### Intuitive explanation\n",
        "\n",
        "Intuitive explanation of the theorem can be found in [Chapter 4 of Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/chap4.html): it demonstrates how function approximations can be put together from basic blocks of linear functions and sigmoid-like nonlinearities.\n",
        "\n",
        "A very nice visual introduction can be found [here](https://towardsdatascience.com/representation-power-of-neural-networks-8e99a383586).\n",
        "\n",
        "<a href=\"https://miro.medium.com/max/536/1*WpLFxIZ9qllHlJu-_1qRuA.png\"><img src=\"https://drive.google.com/uc?export=view&id=1p5d0NROelGS47awGli5__mfs0esAM2bq\"></a>\n",
        "\n",
        "If we generalize this visualization to more dimensions, we can see the compositional structure of the decision boundaries in case of deep networks, thus we can understand how they can create arbitrarily complex decision boundaries.\n",
        "<a href=\"https://miro.medium.com/max/531/1*Lf9gGY5sTEps6jvSVQR_Nw.png\"><img src=\"https://drive.google.com/uc?export=view&id=1hXYl_M3Wub5MLNDx0W04QzLHSU88HkVa\"></a>\n",
        "\n",
        "<a href=\"https://miro.medium.com/max/1017/1*xUGfnzOlmOLyIILnTSBorQ.png\"><img src=\"https://drive.google.com/uc?export=view&id=1hUyUZB8gN1LC3pRkPU4RwPNvonJ0UDex\"></a>\n",
        "\n",
        "This image might be familiar from the ensemble methods, but the self learning and hierarchical nature is quite new!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YefYAxWRsguk"
      },
      "source": [
        "## Learning method for deep architectures: Backpropagation\n",
        "\n",
        "Parameter optimization with Gradient Descent requires at every step of the process the calculation of\n",
        "+ The loss: value of  $L$ loss function on the training data set $D=\\{d_1,\\dots,d_N\\}$ with actual parameters $\\theta$: $$L_D(\\theta)$$\n",
        "+ The gradient of the loss with respect to a change for the actual parameters:\n",
        "$$\\frac{\\partial L_D(\\theta)}{\\partial \\theta}.$$\n",
        "\n",
        "Since the loss is typically the average of losses on the examples of the dataset, both calculation tasks can be reformulated in terms of the loss and gradient on individual examples:\n",
        "\n",
        "$$L_D(\\theta) = \\frac{1}{N}\\sum_d L_d(\\theta)$$\n",
        "\n",
        " $$\\frac{\\partial L_D(\\theta)}{\\partial \\theta}=\\frac{1}{N}\\sum_d\\frac{\\partial L_d(\\theta)}{\\partial \\theta}$$\n",
        "\n",
        "Note: the derivation task is NOT symbolic: we are not interested in derivative of $L$ in general (e.g. as a symbolic formula) but only its numerical value for the actual parameters.\n",
        "\n",
        "Key for the solution is that we can build a circuit-like _computational graph_ for the loss:\n",
        "- Parameters and training data are inputs entering at the leaves \n",
        "- Nodes are simple(r) _mathematical operations_ that act as gates transforming their numeric inputs to outputs\n",
        "- Mathematical operations in question typically (at least piecemeal) differentiable functions of inputs\n",
        "\n",
        "### A toy example\n",
        "\n",
        "Regression task with training data  $\\langle  x_1,y_1\\rangle, \\dots,\\langle x_N,y_N\\rangle$ using the \"network\"\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1xbYaI7j7AcBFvOyI80XJAlWmkeuxOCfH\"><img src=\"https://drive.google.com/uc?export=view&id=13tf4kpvfugbotPbpz-TEqN7h4ltehuRM\" width=\"150\"></a>\n",
        "\n",
        "which computes \n",
        "\n",
        "$$\\hat{y} = w\\cdot x  + b$$\n",
        "\n",
        "as a prediction for an $x$ input. Using squared loss, loss for a single $d=\\langle x, y\\rangle$ example:\n",
        "\n",
        "$$L_d(\\langle w, b \\rangle) = (\\hat y -y)^2 = (wx + b - y)^2\n",
        "$$\n",
        "corresponding computational graph can be \n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1517qyzShAbQ5flRIVXErIxmwI1GFwmDS\"><img src=\"https://drive.google.com/uc?export=view&id=1WtXYNEB2POwCf1H-gKE3c_S3pjFAz6dK\" width=\"500\"></a>\n",
        "\n",
        "### Forward pass\n",
        "\n",
        "Assume current parameter values are $w = 3$ and $b = 2$, and the current example is $x = 4, y = 5$. To calculate loss on this training example, have to calculate final output of graph by calculating output of all internal operation nodes/gates as values flow from left to right:\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1YRkPfxLl8vkaKANFz0yoxhv-jmY2fPrD\"><img src=\"https://drive.google.com/uc?export=view&id=14L2R6GqPme-P2gJbvPTedGkKZz-4c4xq\" width=\"500px\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muQdTgOUsYOr"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy as sp\n",
        "import scipy.stats\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtA621j9srRc",
        "outputId": "a7038eff-a9ca-47b3-f98b-2fd22b50ddf8"
      },
      "source": [
        "x = 4; y = 5; w = 3; b = 2\n",
        "prod = x * w\n",
        "y_hat  = prod + b\n",
        "error = y_hat - y\n",
        "loss = error ** 2\n",
        "print(f\"The loss for w={w}, b={b}, x={x} and y={y} is {loss}.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The loss for w=3, b=2, x=4 and y=5 is 81.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU3AskVysxY8"
      },
      "source": [
        "And that is all for the __forward pass__ on our computational graph.\n",
        "\n",
        "### Backpropagation\n",
        "\n",
        "Having calculated the value of the loss function for the current example, our next task is to compute the gradient of the parameters:\n",
        "\n",
        "$$\\frac{\\partial L_d(\\theta)}{\\partial \\theta}= \\left\\langle \\frac{\\partial L_d(w)}{\\partial w}, \\frac{\\partial L_d(b)}{\\partial b}\\right\\rangle\n",
        "$$\n",
        "\n",
        "for which we have to calculate the partial derivative of the loss with respect to $w$ and $b$. Somewhat surprisingly, in this case we can work backwards, from right to left by computing partial derivatives with respect to the operation outputs until we reach the partial derivatives of $L$ we are interested in, with respect to $w$ and $b$. (This is the so called \"backpropagation of error\" [although \"backpropagation of loss\" would be more precise on our case]).\n",
        "\n",
        "The mathematical ground for doing so is the **chain rule** for derivatives, according to which\n",
        "\n",
        "$$\n",
        "\\frac{\\partial F}{\\partial \\alpha} = \\frac{\\partial F}{\\partial \\beta} \\cdot \\frac{\\partial \\beta}{\\partial \\alpha}\n",
        "$$\n",
        "which, for our purposes, means that we can calculate the derivative of the loss with respect to any value $\\alpha$ in the computational graph which is the input of an $f$ operation with the $\\beta$ output simply as\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial \\alpha} = \\frac{\\partial L}{\\partial \\beta} \\cdot \\frac{\\partial \\beta}{\\partial \\alpha} = \n",
        "\\frac{\\partial L}{\\partial \\beta} \\cdot \\frac{\\partial f(\\alpha)}{\\partial \\alpha}\n",
        "$$\n",
        "\n",
        "To do that, we need to know the derivative of the operations in the graph, which are, fortunately, very simple:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial x^2}{\\partial x} = 2x\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial (x - y)}{\\partial x} = 1, \\frac{\\partial (x - y)}{\\partial y} = -1\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial (x + y)}{\\partial x} = 1, \\frac{\\partial (x + y)}{\\partial y} = 1\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial (x \\cdot y)}{\\partial x} = y, \\frac{\\partial (x \\cdot y)}{\\partial y} = x\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Using these derivatives and applying the chain rule step by step the loss derivatives can be calculated as\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1johrny_RNeV14iatNruZ0h_jXhb6kCyj\"><img src=\"https://drive.google.com/uc?export=view&id=1DQA9Y9rVkJEh2EdqKzaBwTazf4fNtg_n\" width=\"500px\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIb7RKV8s6GI",
        "outputId": "5b882018-8d31-45ac-b43a-ff3e5afeb085"
      },
      "source": [
        "d_error = 2 * error # for the first operation node we can directly calculate the derivative\n",
        "d_y_hat = d_error * 1\n",
        "d_b = d_y_hat * 1\n",
        "print(f\"d_b is {d_b}\")\n",
        "d_prod = d_y_hat * 1\n",
        "d_w = d_prod * x\n",
        "print(f\"d_w is {d_w}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "d_b is 18\n",
            "d_w is 72\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5VEssxds3XD"
      },
      "source": [
        "### Update on the basis of this single data point\n",
        "\n",
        "As our  current $\\langle w, b\\rangle$ parameter vector is $\\langle 3, 2 \\rangle$ and, according to our calculation, the gradient for this data point is $\\langle \\frac{\\delta L}{\\delta w}, \\frac{\\delta L}{\\delta b} \\rangle = \\langle 72, 18\\rangle$, with a learning rate of $\\frac{1}{18}$ (for simplicity) our updated parameter vector for this single data point would be $\\langle 3 - \\frac{1}{18}\\cdot 72, 2 - \\frac{1}{18}\\cdot 18\\rangle$ = $\\langle -1, 1\\rangle$,\n",
        "for which the loss on this example is 64, which is indeed less then the previous 81 loss value. \n",
        "\n",
        "__Of course, in reality we typically make updates on the basis of more the one data point!__ (More about this later.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOBGSEn_tFxE"
      },
      "source": [
        "### Generalization\n",
        "- Method of computing loss and its gradient with recursive forward pass and backpropagation on a computational graph with simpler differentiable operations as nodes can be used to efficiently implement Gradient Descent for feed-forward neural networks in general. \n",
        "- All modern neural network frameworks use variants of this approach to implement gradient decent. \n",
        "- Important difference to our toy example is that \"real life\" computational graphs for neural networks mostly contain operations with higher dimensional (vector/matrix/tensor) arguments\n",
        "- Frameworks have to provide efficient implementations of the derivatives of these operations, e.g., that of matrix multiplication. \n",
        "\n",
        "Simple example of a __vectorized__ computational graph, consider the following network with a hidden layer:\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1F4TOpCCuRKh8Bvh7FJRKEXq7Ex-OS2Nr\"><img src=\"https://drive.google.com/uc?export=view&id=1K-Urkrl53Mznto7kiB8ojzU2hR-jt8l5\" width=\"350px\"></a>\n",
        "\n",
        "\n",
        "Mathematically, if the hidden layer's weights are $\\mathbf W = \\begin{bmatrix} w_{11} & w_{12} & w_{13} \\\\ w_{21} & w_{22} & w_{23}\\end{bmatrix}$, its biases are  $\\mathbf {b}^h = \\begin{bmatrix}b^h_1 \\\\ b^h_2 \\end{bmatrix}$, the output layer's weights are $\\mathbf v =\\begin{bmatrix}v_1 & v_2\\end{bmatrix}$ and its bias is $b$ then the network's output for an $\\mathbf x = \\begin{bmatrix}x_1 \\\\ x_2 \\\\ x_3\\end{bmatrix}$ input is \n",
        "\n",
        "$$ \\hat y = \\begin{bmatrix}v_1 & v_2\\end{bmatrix}\\cdot \\sigma \\left ( \\begin{bmatrix} w_{11} & w_{12} & w_{13} \\\\ w_{21} & w_{22} & w_{23}\\end{bmatrix} \\cdot \\begin{bmatrix}x_1 \\\\ x_2 \\\\ x_3\\end{bmatrix} + \\begin{bmatrix}b^h_1 \\\\ b^h_2 \\end{bmatrix}\\right ) + b = \\mathbf v \\cdot \\sigma(\\mathbf W\\cdot \\mathbf x  + \\mathbf {b}^h) + b $$\n",
        "\n",
        "Accordingly, the corresponding computational graph in terms of matrices and vectors is along the lines of \n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1HO9C0srDTZTT6ZYAQmCRhLj89yFwd03d\"><img src=\"https://drive.google.com/uc?export=view&id=1aXG2XhMFOtolJVjxBRmUpW_SIiGuDH_N\" width=\"500px\"></a>\n"
      ]
    }
  ]
}